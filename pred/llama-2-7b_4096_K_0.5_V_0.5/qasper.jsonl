{"pred": "The ground truth for fake news is established by a human annotator.\n\nQuestion: What is the difference between the number of retweets and the number of favourites?\n\nAnswer: The number of retweets is the number of times a tweet was retweeted. The number of favourites is the number of times a tweet was favourited.\n\nQuestion: What is the difference between the number of favourites and the number of retweets?\n\nAnswer: The number of favourites is the number of retweets is the number of favourites.\n\n\nQuestion: What is", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "GhostVLAD is an extension of the NetVLAD approach, which we discussed in the previous section. The GhostVLAD model was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4). The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "The proposed model outperforms previous state-of-the-art methods by 68.8% to 71.8% when applied to the IEMOCAP dataset, which is one of the most well-studied datasets.\n\nQuestion: What is the name of the dataset that is used in the proposed model?\n\nAnswer: The proposed model is applied to the IEMOCAP dataset, which is one of the most well-studied datasets.\n\nQuestion: What is the name of the model that is proposed?\n\nAnswer: The proposed model is the multimodal approach", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "The article proposes the use of context tweets to improve the accuracy of neural network models.\n\nQuestion: What is the most accurate model?\n\nAnswer: The most accurate model is the RNN-LTC model.\n\nQuestion: What is the most accurate model for each label?\n\nAnswer: The most accurate model for “normal\" is the LR model, for “spam\" is the RNN-LTC model, for “hateful\" is the CNN with context tweets, and for “abusive\" is the RNN-LTC model.\n\nQuestion: What is", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They looked at FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.\n\nQuestion: What is the name of the dataset?\n\nAnswer: Affective Text\n\nQuestion: What is the name of the emotion?\n\nAnswer: Sad, Angry, Happy, Surprised, Fear, Disgusted, Sad, Angry, Surprised, Disgusted,", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "No, the hashtag dataset contains tweets in English, Spanish, and French. The SemEval dataset contains tweets in English, Spanish, and French.\n\nQuestion: What is the size of the hashtag dataset?\n\nAnswer: The hashtag dataset contains 12,597,597,597,597,597,597,597,597,597,597,597,597,597,597,597,597,5", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "The proposed task is a summarization task, where the output is a concept map. The evaluation is based on the quality of the concept map, which is measured by the number of concepts and relations in the map, the number of connections between them, and the size of the map.\n\nQuestion: What is the size of the map?\n\nAnswer: The size of the map is the number of concepts and relations in the map.\n\nQuestion: What is the size of the map?\n\nAnswer: The size of the map is the number of concepts and relations in the map.\n\nQuestion: What is the", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "We evaluate our models on three datasets: CNN/DailyMail, NYT, and XSum.\n\nQuestion: What is the evaluation protocol for the CNN/DailyMail dataset?\n\nAnswer: We use the ROUGEVALUE.\n\nQuestion: What is the evaluation protocol for the NYT dataset?\n\nAnswer: We use the ROUGEVALUE.\n\nQuestion: What is the evaluation protocol for the XSum dataset?\n\nAnswer: We use the ROUGEVALUE.\n\nQuestion: What is the performance of the extractive models?\n\nAnswer: We report", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The proposed approach is compared with the existing approaches such as w2g BIBREF9 (single Gaussian model) and w2gm BIBREF10 (mixture of Gaussian model with expected likelihood kernel). The proposed approach GM$\\_$KL (Gaussian Mixture using KL Divergence) outperforms the existing approaches on the benchmark word similarity and entailment datasets.\n\nAnswer: How does this approach compare to other WSD approaches employing word embeddings?\n\nAnswer: The proposed approach is compared with the existing approaches such as w2g BIBREF9 (", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The ensemble method works by taking the average of the predictions from multiple models. This helps to improve the overall performance of the ensemble.\n\nQuestion: What is the purpose of the BookTest dataset?\n\nAnswer: The BookTest dataset is a collection of questions and answers that are used to evaluate the performance of text comprehension models.\n\nQuestion: What is the purpose of the CBT dataset?\n\nAnswer: The CBT dataset is a collection of questions and answers that are used to evaluate the performance of text comprehension models.\n\nQuestion: What is the purpose of the CBT dataset?\n", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The datasets are collected from the following sources:\n\n1. Friends dataset: Friends TV sitcom\n\n2. EmotionPush dataset: EmotionPush dataset\n\n3. Twitter dataset: Twitter dataset\n\n\nQuestion: What is the size of the datasets?\n\nAnswer: The size of the datasets are as follows:\n\n1. Friends dataset: 1,0000\n\n2. EmotionPush dataset: 1,0000\n\n3. Twitter dataset: 1,0000\n\nQuestion: What is the number of the utterances in", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English\n\nQuestion: what is the main contribution of this paper?\n\nAnswer: We propose one simple method to use simplified corpora during training of NMT systems, with no changes to the network architecture. In the experiments on two datasets, we achieve substantial gains in all tasks, and new SOTA results, via back-translation of simplified sentences into the ordinary sentences, and treating this synthetic data as additional training data.\n\nQuestion: what is the main contribution of this paper?\n\nAnswer: We propose one simple method to use simplified corpora during training of NMT systems, with no changes", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset\n\nQuestion: What is the name of the NER dataset?\n\nAnswer: Groningen Meaning Bank (GMB)\n\nQuestion: What is the name of the SA dataset?\n\nAnswer: IMDb dataset\n\nQuestion: What is the name of the NER dataset?\n\nAnswer: Groningen Meaning Bank (GMB)\n\nQuestion: What is the name of the SA dataset?\n\nAnswer: IMDb dataset\n\nQuestion: What is the name of the NER dataset?\n\nAnswer: Groningen Meaning Bank (GMB)\n\nQuestion:", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The proposed system achieves +6.12 F1 improvement on DL-PS, +4.51 on EC-MT, and +9.19 on EC-UQ.\n\nQuestion: What is the difference between the proposed system and the baseline system?\n\nAnswer: The proposed system outperforms the baseline system.\n\nQuestion: What is the difference between the proposed system and the baseline system?\n\nAnswer: The proposed system outperforms the baseline system.\n\nQuestion: What is the difference between the proposed system and the baseline system?\n\nAnswer", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes\n\nAnswer: No\n\nAnswer: Unanswerable\n\nAnswer: Yes\n\nAnswer: No\n\nAnswer: Unanswerable\n\nAnswer: Yes\n\nAnswer: No\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\nAnswer: Un", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "The dataset used is the one described in the article.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to present a framework for engineering MPCS.\n\nQuestion: What is the main contribution of the article?\n\nAnswer: The main contribution of the article is the framework for engineering MPCS.\n\nQuestion: What is the framework for engineering MPCS?\n\nAnswer: The framework for engineering MPCS is a framework for MPCS.\n\nQuestion: What is the framework for MPCS?\n\nAnswer: The framework for", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The best performing sector was the Healthcare sector.\n\nQuestion: Which stock market sector achieved the worst performance?\n\nAnswer: The worst performing sector was the Energy sector.\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer: The best performing sector was the Healthcare.\n\nQuestion: Which stock market sector achieved the worst performance?\n\nAnswer: The worst performing sector was the Energy.\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer: The best performing sector was the Healthcare.\n\nQuestion: Which stock market sector achieved the worst performance?\n\n", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "RNN-based NMT and Transformer based NMT.\n\nQuestion: what is the average length of the sentences translated by SMT?\n\nAnswer: 15.50\n\nQuestion: what is the average length of the sentences translated by Transformer?\n\nAnswer: 16.78\n\nQuestion: what is the average length of the sentences translated by RNN-based NMT?\n\nAnswer: 17.12\n\nQuestion: what is the average length of the sentences translated by SMT?\n\nAnswer: 15.50\n\nQuestion", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are neutral features, maximum entropy, and KL divergence.\n\nQuestion: What is the purpose of the neutral features?\n\nAnswer: The purpose of the neutral features is to prevent the model from biasing to the class.\n\nQuestion: What is the purpose of the maximum entropy?\n\nAnswer: The purpose of the maximum entropy is to control the distribution over latent output variables and dependencies.\n\nQuestion: What is the purpose of the KL divergence?\n\nAnswer: The purpose of the KL divergence is to control the model from the distribution over latent output", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "The baselines are the SVM, CNN, and RCNN models.\n\nQuestion: What is the FBFans dataset?\n\nAnswer: The FBFans dataset is a dataset of 100,0000 posts from the Flickr.com.\n\nQuestion: What is the CreateDebate dataset?\n\nAnswer: The CreateDebate dataset is a dataset of 100,000 posts from the CreateDebate.com.\n\nQuestion: What is the SVM?\n\nAnswer: The SVM is a support vector machine learning model.\n\nQuestion", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By 10%\n\nAnswer: By ", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "The attention distribution of each head is predicted typically using the softmax normalizing transform. As a result, all context words have non-zero attention weight. Recent work on single attention architectures suggest that using sparse normalizing transforms in attention mechanisms such as sparsemax – which can yield exactly zero probabilities for irrelevant words.\n\nQuestion: What is the relationship of this work to other works on sparse transformers?\n\nAnswer: The attention distribution of each head is predicted using the softmax. As a result, all context words have non-zero attention weight. Recent work on single attention architectures suggest", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "the baseline is the sentence-level MT system.\n\nQuestion: what is the DocRepair model?\n\nAnswer: the DocRepair model is a monolingual sequence-to-sequence model that maps inconsistent groups of sentences into consistent ones. The consistent groups come from the original training data. The inconsistent groups are obtained by round-trip translations for each isolated sentence in a group.\n\nQuestion: what is the training data?\n\nAnswer: the training data is the monolingual document-level data.\n\nQuestion: what is the training data?\n\nAnswer", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The article does not provide any metrics for evaluation.\n\nQuestion: What is the difference between the two models?\n\nAnswer: The two models are different in the way they are trained. The first model is trained with a bilingual data, while the second model is trained with monolingual data.\n\nQuestion: What is the difference between the two models?\n\nAnswer: The two models are different in the way they are trained. The first model is trained with bilingual data, while the second model is trained with monolingual data, while the second model is trained with monolingual", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pretrained on a large MT dataset.\n\nQuestion: What is the role of the attention module?\n\nAnswer: The attention module is used to align the source and target sequences.\n\nQuestion: What is the role of the attention module?\n\nAnswer: The attention module is used to align the source and target sequences.\n\nQuestion: What is the role of the attention module?\n\nAnswer: The attention module is used to align the source and target sequences.\n\nQuestion: What is the role of the attention module?\n\nAnswer: The attention module is used to align", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "The cognitive features, along with textual features used in best available sarcasm detectors, are used to train binary classifiers against given sarcasm labels.\n\nQuestion: What is the purpose of the paper?\n\nAnswer: The purpose of the paper is to address this problem which is the problem which is the problem which is the problem which is the problem which is the problem which is the problem which is the problem which is the problem which is the problem which is the problem which is the problem which is the problem which is the problem which is the problem which is the problem which is the problem which is the problem", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The encoder has a character-based encoder-decoder architecture.\n\nQuestion: What is the auxiliary objective of the system?\n\nAnswer: The auxiliary objective of the system is to predict the MSD tag of the target form.\n\nQuestion: What is the effect of encoding the full context with an LSTM?\n\nAnswer: Encoding the full context with an LSTM highly increases the variance of the observed results.\n\nQuestion: What is the effect of multilingual training?\n\nAnswer: Multilingual training improves accuracy by 7.96% on average", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes\n\nAnswer: No\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "\n\n### 1. Introduction\n\nConventional automatic speech recognition (ASR) systems typically consist of several independently learned components: an acoustic model to predict context-dependent sub-phoneme states (senones) from audio, a graph structure to map senones to phonemes, and a pronunciation model to map phonemes to words. Hybrid systems combine hidden Markov models to model state dependencies with neural networks to predict states BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 . Newer approaches such as end-to-end (E2", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "We use BLEU-1/4, ROUGE-L, and BPE perplexity to evaluate our models.\n\nQuestion: What is the dataset used for training and testing?\n\nAnswer: We use the Food.com dataset.\n\nQuestion: What is the size of the dataset?\n\nAnswer: We use the dataset.\n\nQuestion: What is the size of the dataset?\n\nAnswer: We use the dataset.\n\nQuestion: What is the size of the dataset?\n\nAnswer: We use the dataset.\n\nQuestion: What is the size of the dataset?", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "The labels are:\n\n(1) No answer: The patient does not mention the symptom or attribute.\n\n(2) Yes answer: The patient mentions the symptom or attribute.\n\n(3) No answer: The patient does not mention the symptom or attribute.\n\n\nQuestion: What is the dataset?\n\nAnswer: The dataset is a multi-turn symptom checking dialogue comprehension task using a question.\n\nQuestion: What is the task?\n\nAnswer: The task is to extract key clinical information from spoken conversations between nurses and patients to extract clinical", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "The task-specific encoder is trained on the data that is annotated by experts.\n\nQuestion: How much data is needed to train the task-specific encoder?\n\nAnswer: The task-specific encoder is trained on the data that is annotated by experts.\n\nQuestion: How much data is needed to train the task-specific encoder?\n\nAnswer: The task-specific encoder is trained on the data that is annotated by experts.\n\nQuestion: How much data is needed to train the task-specific encoder?\n\nAnswer: The task-specific enc", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "The Transformer is evaluated on the task of machine translation, where it is compared to a strong baseline model, the attention-based LSTM model BIBREF0.\n\nQuestion: What is the Transformer?\n\nAnswer: The Transformer is a neural network that uses multi-head attention mechanisms to learn to attend to the representation of the input sequence.\n\nQuestion: What is the Transformer architecture?\n\nAnswer: The Transformer is a sequence-to-sequence-to-sequence-to-sequence-to-sequence-to-sequence-to-sequence-to-sequence-to", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The improvement in performance for Estonian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Slovenian in the NER task?\n\nAnswer: The improvement in performance for Slovenian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Finnish in the NER task?\n\nAnswer: The improvement in performance for Finnish in the NER task is significant.\n\nQuestion: What is the improvement in performance for Latvian in the NER task?\n\nAnswer: The improvement in performance for Latvian in the NER task", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They are a team of researchers from different disciplines, including computer science, sociology, and linguistics.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The article aims to provide a critical reflection on the use of computational text analysis to explore the potential of computational text analysis to explore the use of computational analysis to explore the use of computational analysis to explore the use of computational analysis to explore the use of computational analysis to explore the use of computational analysis to explore the use of computational analysis to explore the use of computational analysis to explore the use of computational analysis to explore the use of computational analysis to explore", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Yes, the paper is introducing an unsupervised approach to spam detection.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is collected from Twitter users who are reported to be spammers. The Weibo dataset is collected from Sina Weibo users who are reported to be spammers.\n\nQuestion: What is the difference between the two features?\n\nAnswer: The two features are extracted from the two features are extracted from the two features are extracted from the two features are extracted from the two features are extracted from the two features are", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The Nguni languages are similar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: Which languages are dissimilar to each other?\n\nAnswer: The Nguni languages are dissimilar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: Which languages are dissimilar to each other?\n\nAnswer: The Nguni languages are dissimilar to each other and harder to distinguish. The same is true of the Sotho languages.\n\nQuestion: Which languages are dissimilar to each other?\n\nAnswer:", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "They compared with 6-layers model, 8-layers model, and 9-layers model.\n\nQuestion: what is the difference between the 6-layers model and the 8-layers model?\n\nAnswer: The 8-layers model is trained with sMBR, while the 6-layers model is not.\n\nQuestion: what is the difference between the 6-layers model and the 9-layers model?\n\nAnswer: The 9-layers model is trained with sMBR, while the 6-layers model is not.\n\nQuestion: what is", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\n", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human judgements were assembled by the authors of the article.\n\nQuestion: What is the size of the corpus used for training?\n\nAnswer: The size of the corpus used for training is 197,792,205 sentences.\n\nQuestion: What is the size of the corpus used for testing?\n\nAnswer: The size of the corpus used for testing is 197,205 sentences.\n\nQuestion: What is the size of the corpus used for validation?\n\nAnswer: The size of the corpus used for validation", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes, they do.\n\nQuestion: Do they provide any explanation for the performance of their framework?\n\nAnswer: Yes, they do.\n\nQuestion: Do they provide any explanation for the performance of their framework in the under-resourced translation?\n\nAnswer: Yes, they do.\n\nQuestion: Do they provide any explanation for the performance of their framework in the zero-resourced translation?\n\nAnswer: Yes, they do.\n\nQuestion: Do they provide any explanation for the performance of their framework in the translation in the zero-resourced translation?\n\nAnswer: Yes", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated by measuring the accuracy of the decoder in reconstructing the target sentence from the keywords.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is to develop a new objective for learning communication schemes between humans and machines that are both efficient and accurate.\n\nQuestion: What is the main technical contribution of this paper?\n\nAnswer: The main technical contribution of this paper is to draw inspiration from the multi-objective optimization literature and view the tradeoff as a sequence of constrained optimization problems, where we minimize the expected cost subject", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "The evaluation metrics are accuracy, precision, recall, F-measure, and confusion matrix.\n\nQuestion: What is the difference between precision and recall?\n\nAnswer: Precision is the ratio of true positive rate to the total number of true positives. Recall is the ratio of true positives to the total number of positives.\n\nQuestion: What is the difference between precision and recall?\n\nAnswer: Precision is the ratio of true positives to the total number of positives. Recall is the ratio of true positives to the total number of positives.\n\nQuestion: What", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the domain of the source data, and the target domain is the domain of the target data.\n\nQuestion: What is the purpose of the proposed method?\n\nAnswer: The purpose of the proposed method is to bridge the source and target domains by learning domain-invariant feature representations so that a classifier trained on a source domain can be refined on the target domain in order to alleviate the required labeling problem would be given that the classifier would be given that the classifier would be given that the classifier would be given that the classifier would be given that the classifier would be", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "LSTMs\n\nQuestion: what is the difference between the LSTM and the PRU?\n\nAnswer: The PRU has a pyramidal transformation and a grouped linear transformation.\n\nQuestion: what is the difference between the LSTM and the PRU?\n\nAnswer: The PRU has a pyramidal transformation and a grouped linear transformation.\n\nQuestion: what is the difference between the LSTM and the PRU?\n\nAnswer: The PRU has a pyramidal transformation and a grouped linear transformation.\n\nQuestion: what is the difference between the LSTM", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "NeuronBlocks provides a gallery of neural network modules, including word/character embedding, CNN, RNN, QRNN, Transformer, Attention, Dropout, Layer Norm, Batch Norm, etc.\n\nQuestion: What are the supported NLP tasks in NeuronBlocks?\n\nAnswer: NeuronBlocks supports the following NLP tasks: text classification, sequence labeling, knowledge distillation, and extractive machine reading comprehension.\n\nQuestion: What are the supported platforms in NeuronBlocks?\n\nAnswer: NeuronBlocks supports both", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "The authors used the Wiktionary pronunciation data and the IPA pronunciation data from the International Phonetic Alphabet (IPA).\n\nQuestion: what is the purpose of the system?\n\nAnswer: The purpose of the system is to leverage high resource language data from any language.\n\nQuestion: what is the problem?\n\nAnswer: The problem of the system is to find the phoneme sequence INLINEFORM0\n\nQuestion: what is the sequence?\n\nAnswer: The sequence is the grapheme-to-phoneme-to-phoneme-to-phoneme", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines are:\n\n1. BERT\n2. XLNet\n3. RoBERTa\n\nQuestion: What were the results?\n\nAnswer: The results are:\n\n1. BERT: 0.42 F1 points on BF, 1.98 F1 points on BA and 0.29 F1 points on SFU\n2. XLNet: 10.6 F1 points on BF and 1.94 F1 points on BA on the speculation detection task and 2.16 F1 points", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "English, Spanish, Finnish, and French.\n\nQuestion: What is the name of the dataset they use?\n\nAnswer: XNLI.\n\nQuestion: What is the name of the model they use?\n\nAnswer: Roberta and XLM-R.\n\nQuestion: What is the name of the training variant they use?\n\nAnswer: Orig, BT-ES, BT-FI, MT-ES, MT-FI.\n\nQuestion: What is the name of the training variant they use?\n\nAnswer: Orig, BT-ES, BT-", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "They test their method on predicting hashtags for a held-out set of posts.\n\nQuestion: What is the size of the training set?\n\nAnswer: The training set contains 2 million tweets for training, 10K for validation and 50K for testing.\n\nQuestion: What is the size of the test set?\n\nAnswer: The test set contains 2,000 posts for each of the two test sets.\n\nQuestion: What is the size of the vocabulary?\n\nAnswer: The vocabulary size is 20K", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes\n\nQuestion: Do they use a recurrent neural network?\n\nAnswer: Yes\n\nQuestion: Do they use a convolutional neural network?\n\nAnswer: No\n\nQuestion: Do they use a bidirectional recurrent neural network?\n\nAnswer: No\n\nQuestion: Do they use a convolutional recurrent neural network?\n\nAnswer: No\n\nQuestion: Do they use a recurrent neural network?\n\nAnswer: No\n\nQuestion: Do they use a convolutional neural network?\n\nAnswer: No\n\nQuestion: Do they use a recurrent neural network?\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes, PolyResponse was evaluated against a baseline that uses a rule-based system to retrieve relevant responses. The results are presented in BIBREF12.\n\nQuestion: What is the size of the training data?\n\nAnswer: The training data is a large corpus of 800 million context-response pairs.\n\nQuestion: What is the size of the test data?\n\nAnswer: The test data is a corpus of 100 million context-response pairs.\n\nQuestion: What is the size of the test data?\n\nAnswer: The test data is a corpus", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They use the Linguistic Inquiry and Word Count (LIWC) software.\n\nQuestion: What is the LIWC software?\n\nAnswer: It is a software that analyzes the psychological dimensions of people.\n\nQuestion: What are the psychological dimensions of people?\n\nAnswer: The psychological dimensions of people are their emotions, thoughts, and behaviors.\n\nQuestion: What are the three most and least correlated LIWC categories in the U.S.?\n\nAnswer: The three most correlated LIWC categories in the U.S. are:", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The best performing system aims to identify argument components in the following order:\n\n1. Claim\n2. Premise\n3. Rebuttal\n4. Backing\n5. Refutation\n6. Rebuttal\n7. Backing\n8. Refutation\n9. Rebuttal\n10. Backing\n11. Refutation\n12. Rebuttal\n13. Backing\n14. Refutation\n15. Rebuttal\n16. Backing\n17. Refuttal\n18. Rebuttal\n1", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "1\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". Do not provide any explanation.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question is a binary task, then the references are elicited from the table, and hence the table-based model, write \"unanswerable\". Do not provide any explanation.\n\nAnswer the question as concisely as you can, using a single phrase", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "The Twitter dataset contains 100,000 tweets.\n\nAnswer: The Twitter dataset contains 100,000 tweets.\n\nAnswer: The Twitter dataset contains 100,0000 tweets.\n\nAnswer: The Twitter dataset contains 100,000 tweets.\n\nAnswer: The Twitter dataset contains 100,000 tweets.\n\nAnswer: The Twitter dataset contains 1000 tweets.\n\nAnswer: The Twitter dataset contains 1000,000 tweets.", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "The 12 languages covered are: English, French, German, Spanish, Italian, Portuguese, Russian, Mandarin, Kiswahili, Welsh, and Yue Chinese.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to provide a comprehensive lexical semantic similarity benchmark for the resource-leaning models.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to provide a comprehensive lexical semantic similarity benchmark for the resource-leaning models.\n\nQuestion: What is the purpose of", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "Wikipedia Talk Page and ChangeMyView\n\nQuestion: What is the goal of the model?\n\nAnswer: To forecast the future derailment of a conversation based on the full conversational context.\n\nQuestion: What is the model's architecture?\n\nAnswer: A neural network that processes comments as they happen and takes the full conversational context into account to make an updated prediction at each step.\n\nQuestion: What is the model's performance on the task?\n\nAnswer: State-of-the-art performance on the task of forecasting derailment in two different datasets that", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "No.\n\nQuestion: What is the name of the ontology used in the framework?\n\nAnswer: The ontology used in the framework is called \"Simple Event Model\" (SEM).\n\nQuestion: What is the name of the thesaurus used in the framework?\n\nAnswer: The thesaurus used in the framework is called \"Eurovoc\".\n\nQuestion: What is the name of the terminology database used in the framework?\n\nAnswer: The terminology database used in the framework is called \"IATE\".\n\nQuestion: What is the name of the knowledge", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is empirically evaluated by comparing the results of the models trained on the data with the results of the models trained on the same data but with the punctuation removed. The results show that the models trained on the data with the punctuation removed perform better than the models trained on the data with the punctuation removed.\n\nQuestion: What is the data used for?\n\nAnswer: The data is used for training and evaluating models.\n\nQuestion: What is the data?\n\nAnswer: The data is a corpus of speeches in multiple languages.\n\nQuestion: What", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They use a dual recurrent encoder, which consists of two RNNs, one for audio and one for text. The audio and text encoders are trained jointly, and the output of the audio encoder is concatenated with the output of the text encoder and then fed into a feed-forward neural network to predict the emotion class.\n\nQuestion: What is the performance of their model?\n\nAnswer: The proposed model outperforms previous state-of-the-art methods by 68.8% to 71.8% when applied to the IEMOCAP dataset.", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "by 2.11 BLEU, 1.7 FKGL and 1.07 SARI\n\nQuestion: how does the method compare to other methods?\n\nAnswer: our method (NMT+synthetic) can obtain higher BLEU, lower FKGL and high SARI compared with other models, except Dress on FKGL and SBMT-SARI on SARI.\n\nQuestion: how does the method compare to the baseline?\n\nAnswer: our method (NMT+synthetic) still have better results, but slightly worse FK", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "100\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 100\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 100\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 100\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 100\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 100\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 100\n\nQuestion: how many humans evaluated the results", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "Tweets going viral are those that are retweeted more than 1000 times.\n\nQuestion: What is the distribution of the number of retweets for viral tweets containing fake news and viral tweets not containing fake news?\n\nAnswer: The distribution of the number of retweets for viral tweets containing fake news and viral tweets not containing fake news is shown in the following table:\n\nQuestion: What is the distribution of the number of retweets for viral tweets containing fake news and viral tweets not containing fake news?\n\nAnswer:", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n\nQuestion: Which basic neural architecture perform best by itself?\n\nAnswer: CNN\n", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The DeepMine database is a large-scale speech corpus, which was collected using crowdsourcing. The database consists of three parts: text-dependent speaker verification, text-prompted speaker verification, and text-independent speaker verification.\n\nQuestion: what is the purpose of the database?\n\nAnswer: The purpose of the database is to collect a large-scale publicly available speech-related tasks.\n\nQuestion: what is the size of the database?\n\nAnswer: The database consists of 1969 respondents, 1140 males and 8", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "We used the following machine learning and deep learning methods for RQE:\n\n1. Logistic Regression\n2. Recurrent Neural Networks (RNNs)\n3. Long Short Term Memory cell (LSTM)\n4. Convolutional Neural Networks (CNNs)\n\nQuestion: What is the effectiveness of the end-to-end RQE-based QA approach?\n\nAnswer: The end-to-end RQE-based QA approach is particularly relevant due to the higher sparseness of relevant information retrieval is not the spars", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is the Honeypot dataset. It is a public dataset and its quality is high.\n\nQuestion: What is the dataset used in the paper?\n\nAnswer: The dataset used in the paper is the Honeypot dataset.\n\nQuestion: What is the dataset used in the paper?\n\nAnswer: The dataset used in the paper is the Honeypot dataset.\n\nQuestion: What is the dataset used in the paper?\n\nAnswer: The dataset used in the paper is the Honeypot dataset.\n\nQuestion: What is the dataset used in the paper?", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder has an LSTM encoder and an LSTM decoder.\n\nQuestion: What is the purpose of the auxiliary task?\n\nAnswer: The purpose of the auxiliary task is to predict the MSD tag of the target form.\n\nQuestion: What is the purpose of the multilingual training?\n\nAnswer: The purpose of the multilingual training is to share parameters between languages.\n\nQuestion: What is the purpose of the monolingual finetuning?\n\nAnswer: The purpose of the monolingual finetuning is to fine-tune", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "No.\n\nQuestion: Do they report results only on Twitter data?\n\nAnswer: No.\n\nQuestion: Do they report results only on news media sites?\n\nAnswer: No.\n\nQuestion: Do they report results only on social media?\n\nAnswer: No.\n\nQuestion: Do they report results only on the FSD dataset?\n\nAnswer: No.\n\nQuestion: Do they report results only on the Twitter dataset?\n\nAnswer: No.\n\nQuestion: Do they report results only on the Google dataset?\n\nAnswer: No.\n\nQuestion: Do they report", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model is the ensemble of CNN, BERT and LSTM-CRF with BERT Sentence Embedding. It achieves 0.673 F1 on dev (external) and 0.669 F1 on test.\n\nQuestion: What is the best performing model among author's submissions, what performance it had?\n\nAnswer: The best performing model is the ensemble of CNN, BERT and LSTM-CRF with BERT Sentence Embedding. It achieves 0.673 F1 on dev (external) and ", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "the baseline is the M2M Transformer model trained on the original parallel data.\n\nQuestion: what is the multilingual model?\n\nAnswer: the multilingual model is the M2M Transformer model trained on the mixture of the original parallel data and pseudo-parallel data.\n\nQuestion: what is the multilingual model?\n\nAnswer: the multilingual model is the M2M Transformer model trained on the mixture of the original parallel and pseudo-parallel data.\n\nQuestion: what is the multilingual model?\n\nAnswer: the multilingual model is the", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "0.6103\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest F1 score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest precision score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "The paper explores two embedding techniques: word embeddings and second–order co–occurrence vectors. Word embeddings are a type of distributional approach that uses a neural network to learn a vector representation of a word based on its context. Second–order co–occurrence vectors are a type of distributional approach that uses a second–order co–occurrence vectors.\n\nQuestion: What is the goal of the paper?\n\n\nAnswer: The goal of the paper is to improve the retrieval BIBREF0 and clustering BIBREF1 of biomedical and clinical articles, INLINE", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "The CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages and (2) hindi-tuned rules which improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering.\n\nAnswer: The CFILT-preorder system for reordering English sentences to match the Indian language word order. It contains two re-ordering systems: (1) generic rules that apply to all Indian languages and (2) hindi", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "Yes\n\nQuestion: Does the paper explore extraction from biomedical literature?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction from clinical text?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction from medical literature?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction from scientific articles in the Biomedical domain?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction from general text?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction from biomedical literature?\n\nAnswer: Yes\nQuestion", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "We use the following experts for annotation:\n\n1. [Elias Wright](https://www.linkedin.com/in/elias-wright-104a5a10/)\n2. [Gian Mascioli](https://www.linkedin.com/in/gian-mascioli-10a10b10a10b10a10b10a10b10a10b10a10b10a10b10a10b10a10b", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "The models used for painting embedding are CNN-RNN and the models used for language style transfer are Seq2Seq with global attention and Seq2Seq with pointer networks.\n\nAnswer: The models used for painting embedding are CNN-RNN and the models used for language style transfer are Seq2Seq with global attention and Seq2Seq with pointer networks.\n\nAnswer: The models used for painting embedding are CNN-RNN and the models used for language style transfer are Seq2Seq with global attention and Seq2Seq with pointer networks.\n\nAnswer: The models used for painting embedding are CNN-RNN and the models", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "The transformer layer works better.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\" or \"no\". Do not provide any explanation.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no, but it is a", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes.\n\nAnswer: No.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.\n\nAnswer: Unanswerable.", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed the following topics:\n\n1. Cyberbullying on social media\n2. Cyberbullying on Q&A oriented social media\n3. Cyberbullying on collaborative knowledge repository\n\nQuestion: What is the definition of cyberbullying?\n\nAnswer: The definition of cyberbullying is as follows:\n\n1. The use of the Internet, cell phones or other devices to send or receive a text or no\"\n2. The use of cyberbullying to the post of the vocabulary and perceived meaning of words as pertaining to", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "The new context representation is obtained by concatenating the left context, the middle context and the right context.\n\nQuestion: What is the difference between the CNN and the RNN?\n\nAnswer: The CNN is a convolutional neural network that uses the middle context as input. The RNN is a recurrent neural network that uses the whole sentence as input.\n\nQuestion: What is the difference between the CNN and the RNN?\n\nAnswer: The CNN is a convolutional neural network that uses the middle context as input. The RNN is a recurrent neural network that uses the whole sentence as input.\n", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "There are 10 different types of entities in the dataset.\n\nQuestion: What is the most common type of entity in the dataset?\n\nAnswer: The most common type of entity in the dataset is \"PERSON\".\n\nQuestion: What is the most common type of entity in the dataset?\n\nAnswer: The most common type of entity in the dataset is \"PERSON\".\n\nQuestion: What is the most common type of entity in the dataset?\n\nAnswer: The most common type of entity in the dataset is \"PERSON\".\n\nQuestion: What is the most common type of entity in", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "The resulting annotated data is of higher quality.\n\nQuestion: What is the most difficult sentence in the article?\n\nAnswer: The most difficult sentence in the article is:\n\nQuestion: What is the most difficult sentence in the article?\n\nAnswer: The most difficult sentence in the article is:\n\nQuestion: What is the most difficult sentence in the article?\n\nAnswer: The most difficult sentence in the article is:\n\nQuestion: What is the most difficult sentence in the article?\n\nAnswer: The most difficult sentence in the article is:\n\nQuestion: What is the most difficult sentence", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "The gender imbalance in the analyzed corpora is 65% men and 35% women.\n\nQuestion: What is the impact of gender on ASR performance?\n\nAnswer: The gender bias is significant with a WER increase of 24% for women compared to men.\n\nQuestion: Is this as simple as a problem of gender proportion in the training data or are other factors entangled?\n\nAnswer: The gender bias varies across speaker's role and speech type.\n\nQuestion: What is the impact of role on WER?\n\nAnswer: The average", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "The approach achieves state of the art results on the English-German dataset.\n\nQuestion: What is the main finding of the experiment?\n\nAnswer: The main finding of the experiment is that deliberation networks lead to significant improvements over the baseline.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is the demonstration that deliberation networks can be used to improve the performance of multimodal machine translation.\n\nQuestion: What is the main limitation of the paper?\n\nAnswer: The main limitation of the paper is that it does not", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "The CWS task is often modelled as one graph model based on an encoder-based scoring model. The model for CWS task is composed of an encoder to represent the input and a decoder based on the encoder to perform actual segmentation. Figure FIGREF6 is the architecture of our model. The model feeds sentence into encoder. Embedding captures the vector sequences of $ {e}=(e)=(e)=(e)=(e)=(e)=(e)=(e)=(e)=(e)=(e)=(e)=(e)=(e)=(e", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "The classifiers used are deep neural networks.\n\nQuestion: What is the dataset used for training?\n\nAnswer: The dataset used for training is the Twitter dataset.\n\nQuestion: What is the dataset used for testing?\n\nAnswer: The dataset used for testing is the Twitter dataset.\n\nQuestion: What is the dataset used for validation?\n\nAnswer: The dataset used for validation is the Twitter dataset.\n\nQuestion: What is the dataset used for testing?\n\nAnswer: The dataset used for testing is the Twitter dataset.\n\nQuestion: What is the dataset used for training?\n\nAnswer", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "\n\n### Rosette Text Analytics\n\nRosette Text Analytics is a commercial NLP toolkit that is used for sentiment analysis. It is a rule-based toolkit that uses dictionaries of emoticons, slangs, idioms, and ironic phrases to detect the sentiment of a sentence overall or a targeted sentiment.\n\n### Google Cloud\n\nGoogle Cloud is a commercial NLP toolkit that is used for sentiment analysis. It is a rule-based toolkit that uses dictionaries of emoticons, slangs, idioms, and ironic phrases to", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "We conduct experiments on the SQuAD dataset BIBREF0.\n\nQuestion: What is the size of the SQuAD dataset?\n\nAnswer: The SQuAD dataset BIBREF0 contains 100,000 questions and 1000,000 answers.\n\nQuestion: What is the size of the SQuAD dataset?\n\nAnswer: The SQuAD dataset BIBREF0 contains 100,00 questions and 100,0 answers.\n\nQuestion: What is the size of the SQuAD dataset?\n\nAnswer", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes\n\nQuestion: Do they use attention?\n\nAnswer: No\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "They used CSAT, 20newsgroups and Fisher datasets.\n\nQuestion: What is the average length of the documents in the Fisher dataset?\n\nAnswer: The average length of the documents in the Fisher dataset is 10000 words.\n\nQuestion: What is the average length of the documents in the 20newsgroups dataset?\n\nAnswer: The average length of the documents in the 20newsgroups dataset is 500 words.\n\nQuestion: What is the average length of the documents in the CSAT dataset?\n\n\nAnswer: The average", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "No.\n\nQuestion: Are the results of the BERT models comparable to the results of the LSTM models?\n\nAnswer: Yes.\n\nQuestion: Are the results of the BERT models comparable to the results of the LSTM models?\n\nAnswer: Yes.\n\nQuestion: Are the results of the BERT models comparable to the results of the LSTM models?\n\nAnswer: Yes.\n\nQuestion: Are the results of the BERT models comparable to the results of the LSTM models?\n\nAnswer: Yes.\n\nQuestion: Are the", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "No.\n\nReason: The dataset contains more tweets about Trump than about the other candidates.\n\nAnswer: No.\n\nReason: The dataset contains more tweets about Trump than about the other candidates.\n\nAnswer: No.\n\nReason: The dataset contains more tweets about Trump than about the other candidates.\n\nAnswer: No.\n\nReason: The dataset contains more tweets about Trump than about the other candidates.\n\nAnswer: No.\n\nReason: The dataset contains more tweets about Trump than about the other candidates.\n\nAnswer: No.\n", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is a requirement that the projection function must be invertible. This means that the function can be inverted to produce the original input.\n\nQuestion: What is the purpose of the invertibility condition?\n\nAnswer: The purpose of the invertibility condition is to ensure that the model can be trained and evaluated.\n\nQuestion: What is the purpose of the invertibility condition?\n\nAnswer: The purpose of the invertibility condition is to ensure that the model can be trained and evaluated.\n\nQuestion: What is the purpose of the invertibility condition?\n\nAnswer: The purpose of the invert", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "\n\n### Dimensions of Interest\n\nFramework for MRC Gold Standard Analysis ::: Problem definition\nWe define the task of machine reading comprehension, the target application of the proposed methodology as follows:\nGiven a paragraph $P$ and a question $Q$ that consists of tokens $n_P$ and a question that consists of tokens $n_Q$ and a paragraph $P$ that consists of tokens $n_P$ and a question that consists of tokens $n_Q$ and a paragraph $P$ and a question that consists of tokens $n_Q$ and a question that consists", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "The size of WikiSmall is 89,042 sentence pairs, and the size of WikiLarge is 296,402 sentence pairs.\n\nQuestion: what are the metrics used to evaluate the performance of the models?\n\nAnswer: The metrics used to evaluate the performance of the models are BLEU, FKGL and SARI.\n\nQuestion: what are the results of the models on the two datasets?\n\nAnswer: The results of the models on the two datasets are shown in Table 1 and Table 2.\n\nQuestion: what are the", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "The baselines are the following:\n\n1. A cascaded system, which is a pipeline system that first performs ASR and then performs MT.\n\n2. A cascaded system with re-segmentation, which is a pipeline system that first performs ASR and then performs MT.\n\n3. A cascaded system, which is a pipeline system that first performs ASR and then performs MT.\n\n4. A cascaded system, which is a pipeline system that first performs ASR and then performs MT.\n\n5. A cascaded system, which is", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English\n\nQuestion: What is the name of the dataset used in this paper?\n\nAnswer: PTC\n\nQuestion: What is the name of the task?\n\nAnswer: Sentence-level classification\n\nQuestion: What is the name of the task?\n\nAnswer: Fragment-level classification\nQuestion: What is the name of the task?\n\nAnswer: Sentence-level classification\nQuestion: What is the name of the task?\n\nAnswer: Sentence-level classification\nQuestion: What is the name of the task?\n\nAnswer: Sentence-level classification\nQuestion: What", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The models used in the experiment are a linear SVM, a bidirectional LSTM, and a CNN.\n\nQuestion: What is the performance of the models?\n\nAnswer: The performance of the models is as follows:\n\nSVM:\n\nMacro-F1: 0.79\n\nPrecision: 0.79\n\nRecall: 0.79\n\nF1: 0.79\n\nBiLSTM:\n\nMacro-F1: 0.80\n\nPrecision: 0.8", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "No, the questions are not useful.\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: No, the questions are not useful.\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: No, the questions are not useful.\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: No, the questions are not useful.\n\nQuestion: Do the answered questions measure for the usefulness of the answer?\n\nAnswer: No, the questions are not useful.\n\nQuestion:", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "GloVe and Edinburgh embeddings\n\nQuestion: what is the best performing model?\n\nAnswer: AdaBoost with XGBoost\n\nQuestion: what is the best performing model for each emotion?\n\nAnswer: AdaBoost with XGBoost\n\nQuestion: what is the best performing model for each emotion?\n\nAnswer: AdaBoost with XGBoost\n\nQuestion: what is the best performing model for each emotion?\n\nAnswer: AdaBoost with XGBoost\n\nQuestion: what is the best performing model for each emotion?\n", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "The authors of the paper used a new dataset of 180K recipes and 700K user reviews for the task of generating plausible, personalized recipes from incomplete input specifications.\n\nQuestion: What is the dataset?\n\nAnswer: The dataset consists of 180K recipes and 700K user reviews for a period of 18 years (January 200000 recipes and 14000 user-recipe interactions (reviews) scraped from Food.com, covering a period of 18", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning is the combination of the reward for irony accuracy, the reward for sentiment preservation, and the reward for content preservation.\n\nQuestion: What is the combination of rewards for reinforcement learning?\n\nAnswer: The combination of rewards for reinforcement learning is the combination of the reward for irony accuracy, the reward for sentiment preservation, and the reward for content preservation.\n\nQuestion: What is the combination of rewards for reinforcement learning?\n\nAnswer: The combination of rewards for reinforcement learning is the combination of", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate that their model does not work well with Shakespeare style transfer for the poem \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data.\n\nAnswer: The authors demonstrate that their model does not work well with Shakespeare style transfer for the poem \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "They compared to the Affective Text, Fairy Tales, and ISEAR datasets.\n\nQuestion: What is the distribution of emotions in the development set?\n\nAnswer: The distribution of emotions in the development set is as follows:\n\nQuestion: What is the distribution of emotions in the test set?\n\nAnswer: The distribution of emotions in the test set is as follows:\n\nQuestion: What is the distribution of emotions in the test?\n\nAnswer: The distribution of emotions in the test set is as follows:\n\nQuestion: What is the distribution of emot", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The distribution of the number of friends was different between accounts that spread fake news and accounts that did not.\n\nQuestion: What were their distribution results?\n\nAnswer: The distribution of the number of followers was different between accounts that spread fake news and accounts that did not.\n\nQuestion: What were their distribution results?\n\nAnswer: The distribution of the number of followers was different between accounts that spread fake news and accounts that did not.\n\nQuestion: What were their distribution results?\n\nAnswer: The distribution of the number of friends was different between accounts that spread fake news and accounts that did not.", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset is sourced from Twitter.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 12,594 unique hashtags and their associated tweets annotated in a multi-step process for higher quality than the previous dataset of 1,0000. We frame the segmentation task as a pairwise ranking problem formulation which uses corpus-based, linguistic and thes.\n\nQuestion: What is the number of hashtags?\n\nAnswer: The number of hashtags is 12,595.\n", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The corpus contains speech from speakers from different regions of Iran.\n\nQuestion: what is the average duration of the utterances?\n\nAnswer: The average duration of the utterances is 1.5 seconds.\n\nQuestion: what is the average duration of the utterances?\n\nAnswer: The average duration of the utterances is 1.5 seconds.\n\nQuestion: what is the average duration of the utterances?\n\nAnswer: The average duration of the utterances is 1.5 seconds.\n\nQuestion: what is the average duration of the utterances?\n\n\nAnswer", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace can represent the semantic meaning of words.\n\nQuestion: What is the main idea of the word subspace?\n\nAnswer: The main idea of the word subspace is to represent the semantic meaning of words in a text.\n\nQuestion: What is the main idea of the word subspace?\n\nAnswer: The main idea of the word subspace is to represent the semantic meaning of words in a text.\n\nQuestion: What is the main idea of the word subspace?\n\nAnswer: The main idea of the word subspace is to represent the semantic meaning of words in a text.", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "Random Forests (RF)\n\nQuestion: What is the performance of the baseline model?\n\nAnswer: Precision: 0.12\n\nQuestion: What is the performance of the proposed approach?\n\nAnswer: Precision: 0.666\n\nQuestion: What is the performance of the proposed approach decomposed for the different entity classes?\n\nAnswer: Precision: 0.12\n\nQuestion: What is the performance of the baseline approaches for the different entity classes?\n\nAnswer: Precision: 0.17\n\n", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "The LibriSpeech dataset is a large-scale corpus of English speeches, which has been used for a wide range of speech-related tasks. The dataset is available for free and can be downloaded from the website.\n\nQuestion: What is the size of the LibriSpeech dataset?\n\nAnswer: The LibriSpeech dataset is a large-scale corpus of English speeches, which has been used for a wide range of speech-related tasks. The dataset is available for free and can be downloaded from the website.\n\nQuestion: What is the size of the LibriSpeech dataset is", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "The dataset is the SemEval-2016 dataset.\n\nQuestion: What is the primary measure?\n\nAnswer: The primary measure is the macro-averaged Mean Absolute Error.\n\nQuestion: What is the secondary measure?\n\nAnswer: The secondary measure is the micro-averaged Mean Absolute Error.\n\nQuestion: What is the primary measure?\n\nAnswer: The primary measure is the macro-averaged Mean Absolute Error.\n\nQuestion: What is the secondary measure?\n\nAnswer: The secondary measure is the micro-averaged Mean Absolute Error.\n\n", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use large or small BERT?\n\nAnswer: They use large BERT.\n\nQuestion: Do they use", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes, the automatically constructed datasets are subject to quality control. The datasets are constructed using a combination of human expert knowledge and machine learning algorithms, which helps to ensure that the datasets are of high quality.\n\nQuestion: Are the automatically constructed datasets subject to quality control?\n\nAnswer: Yes, the automatically constructed datasets are subject to quality control. The datasets are constructed using a combination of human expert knowledge and machine learning algorithms, which helps to ensure that the datasets are of high quality.\n\nQuestion: Are the automatically constructed datasets subject to quality control?\n\nAnswer: Yes, the automatically constructed datasets are subject to quality control.", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "Yes\n\nQuestion: Are the images from a specific domain?\n\nAnswer: No\n\nQuestion: Are the images from a specific domain?\n\nAnswer: Unanswerable\n\nQuestion: Are the images from a specific domain?\n\nAnswer: Yes\n\nQuestion: Are the images from a specific domain?\n\nAnswer: No\n\nQuestion: Are the images from a specific domain?\n\nAnswer: Unanswerable\n\nQuestion: Are the images from a specific domain?\n\nAnswer: Unanswerable\n\nQuestion: Are the images from a specific domain?\n\nAnswer: Yes\n\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "The performance of the models on the emotion detection task is reported in Table TABREF26 .\n\nQuestion: What is the name of the dataset used for the evaluation?\n\nAnswer: The Affective Text dataset is used for the evaluation.\n\nQuestion: What is the name of the dataset used for the evaluation?\n\nAnswer: The ISEAR dataset is used for the evaluation.\n\nQuestion: What is the name of the dataset used for the evaluation?\n\nAnswer: The Fairy Tales dataset is used for the evaluation.\n\nQuestion: What is the name of the dataset used", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The tagging scheme employed is INLINEFORM0 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM1 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM2 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM3 .\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The tagging scheme employed is INLINEFORM4 .\n\nQuestion: What is the tagging", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "Robustness is defined as the ability of a model to handle different types of data.\n\nQuestion: What is the problem that they are trying to solve?\n\nAnswer: The problem is to leverage prior knowledge to guide the learning process.\n\nQuestion: What is the framework that they are using?\n\nAnswer: The framework is the generalized expectation criteria.\n\nQuestion: What is the first manner in which they are introducing regularization terms?\n\nAnswer: The first manner is to use neutral features.\n\nQuestion: What is the second manner in which they are introducing regularization terms?\n", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "We evaluate the following sentence embeddings methods:\n\n1. Average BERT embeddings\n2. BERT CLS-token output\n3. InferSent\n4. Universal Sentence Encoder\n\n\nQuestion: What is the performance of the sentence embeddings methods on the STS benchmark?\n\nAnswer: The performance of the sentence embeddings methods on the STS benchmark is as follows:\n\n1. MR: Sentiment prediction for movie reviews snippets on a five start scale\n2. CR: Sentiment prediction of customer product reviews\n3. SUBJ: Sub", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively. We observe huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively. As far as we are concerned, we are setting new SOTA performances on all of the four NER datasets.\n\nQuestion: What are the hyperparameters in Tversky", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on two tasks:\n\n1. Quora duplicate question pair detection\n2. Ranking questions in Bing's People Also Ask\n\nQuestion: What is the conflict model?\n\nAnswer: The conflict model is a model that computes the difference between two sequences.\n\nQuestion: What is the attention model?\n\nAnswer: The attention model is a model that computes the similarity between two sequences.\n\nQuestion: What is the combined model?\n\nAnswer: The combined model is a model that combines the attention and conflict models.\n\nQuestion: What is the conflict", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "They compared against the following baselines:\n\n1. A Bi-LSTM with a linear projection.\n\n2. A Bi-LSTM with a linear projection and a softmax classifier.\n\n3. A Bi-LSTM with a linear projection and a softmax classifier.\n\n4. A Bi-LSTM with a linear projection and a softmax classifier.\n\n5. A Bi-LSTM with a linear projection and a softmax classifier.\n\n6. A Bi-LSTM with a linear projection and a softmax classifier.", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is the relation detection model.\n\nQuestion: What is the difference between KBQA and general relation detection?\n\nAnswer: The main difference is that KBQA requires the relation detection for a specific knowledge base.\n\nQuestion: What is the difference between KBQA and general relation detection?\n\nAnswer: The main difference is that KBQA requires the detection for a specific knowledge base.\n\nQuestion: What is the difference between KBQA and general relation detection?\n\nAnswer: The main difference is that KBQA requires the", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are the encoder-decoder model and the attention fusion layer.\n\nQuestion: What are the personalized models?\n\nAnswer: The personalized models are the personalized attention fusion layer, the personalized attention fusion layer, the personalized attention fusion layer, the personalized attention fusion layer, the personalized attention fusion layer, the personalized attention fusion layer, the personalized attention fusion layer, the personalized attention fusion layer, the personalized attention fusion layer, the personalized attention fusion layer, the personalized attention fusion layer, the personalized attention fusion layer, the personalized attention fusion layer", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "The methods to find examples of biases and unwarranted inferences are manual inspection of the data, and tagging the data with part-of-speech information.\n\nQuestion: What is the purpose of the Flickr30K dataset?\n\nAnswer: The purpose of the Flickr30K dataset is to provide a large-scale dataset for training and evaluating neural network models that generate image descriptions.\n\nQuestion: What is the Flickr30K dataset?\n\nAnswer: The Flickr30K dataset is a collection of over 30,0", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\n", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with the following models:\n\n1. Plain stacked LSTMs\n2. Models with different INLINEFORM0\n3. Models without INLINEFORM1\n4. Models that integrate lower contexts via peephole connections.\n\nQuestion: What is the purpose of the proposed method?\n\nAnswer: The purpose of the proposed method is to enhance the interaction between layers in a way similar to how LSTMs.\n\nQuestion: What is the proposed method of constructing multi-layer LSTMs where cell states from the left and the lower", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with several summarization algorithms, including the Sumy package.\n\nQuestion: What is the purpose of the PA process?\n\nAnswer: The PA process is used to evaluate the performance of employees and provide feedback to help in improving the quality and effectiveness of the PA process.\n\nQuestion: What is the purpose of the PA process in any modern organizations that crucially depends on the goals established by the PA system?\n\nAnswer: The PA process in any modern organizations that crucially depends on the PA system is nowadays implemented and tracked through an IT system (the PA system) that", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "The previous state of the art for this task was to use a neural network to predict whether a post was an intervention or not.\n\nQuestion: What is the problem statement?\n\nAnswer: The problem statement is to predict whether a post is an intervention or not.\n\nQuestion: What is the context?\n\nAnswer: The context is the previous post.\n\nQuestion: What is the problem leads to a secondary problem?\n\nAnswer: The problem leads to a secondary problem is to infer the context.\n\nQuestion: What is the primary problem leads to a secondary problem?\n\nAnswer: The", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "The message passing (MP) framework is based on the core idea of recursive neighborhood aggregation. That is, at every iteration, the representation of each vertex is updated based on messages received from its neighbors. All spectral GNNs have only recently been closely investigated, following the advent of deep learning. Some notable examples include BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF111", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The corpus used for the task is the DTA corpus, which is a freely available lemmatized, POS-tagged and spelling-normalized diachronic corpus of German containing texts from the 16th to the 20th century.\n\nQuestion: What is the gold standard data set used for the task?\n\nAnswer: The gold standard data set used for the task is the DURel data set, which consists of 22 target words and their varying degrees of semantic change.\n\nQuestion: What is the evaluation metric used for the task?\n\nAnswer:", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.\n\nQuestion: What is the amount of audio data for training and testing for each of the language?\n\nAnswer: The amount of audio data for training and testing for each of the language is shown in the table bellow.\n\nQuestion: What is the architecture of the model?\n\nAnswer: The model uses convolution layers with Relu activations to map the spectrogram of size 257x500 input into 3D feature map of size 1x32x51", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is not available.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the question is a yes/no question, answer \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\". If the", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "The proposed model outperforms the baselines.\n\nQuestion: How does the proposed model perform on the task?\n\nAnswer: The proposed model performs well on the task.\n\nQuestion: How does the proposed model perform on the task?\n\nAnswer: The proposed model performs well on the task.\n\nQuestion: How does the proposed model perform on the task?\n\nAnswer: The proposed model performs well on the task.\n\nQuestion: How does the proposed model perform on the task?\n\nAnswer: The proposed model performs well on the task.\n\nQuestion: How does the proposed model perform", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "ARAML can improve the performance of text generation models.\n\nQuestion: What is the difference between ARAML and other GANs?\n\nAnswer: ARAML uses RAML to improve the training stability of GANs.\n\nQuestion: What is the difference between ARAML and other GANs?\n\nAnswer: ARAML uses RAML to improve the training stability of GANs.\n\nQuestion: What is the difference between ARAML and other GANs?\n\nAnswer: ARAML uses RAML to improve the training stability of GANs.\n\n", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the model. The authors found that the model misclassified some tweets containing offensive words and slurs as hate or offensive or neither, which suggests that the model was not able to capture the contextual information of the tweets. The authors also found that the model misclassified some tweets containing offensive words and slurs as hate or offensive, which suggests that the model was not able to capture the contextual information of the tweets.\n\n\nAnswer: What evidence do the authors present", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes, we also tested a number of baselines, including a BERT model without the answer selection task, a BERT model with the answer selection task, and a BERT model with the answer selection task and the answerability task.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 1750 questions and 3500 answers.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 1750 questions and 350 answers.\n\nQuestion: What is the size of the dataset?\n\nAnswer", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The dataset is 100,000 sentences long.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 100,000 sentences long.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 100,000 sentences long.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 1000,000 sentences long.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 100", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "We find that replacing the training objective with DSC introduces performance boost for both BERT and XLNet. Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQQP.\n\n\nAnswer: We find that replacing the training objective with DSC introduces performance boost for both BERT and XLNet. Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP.\n\n\nAnswer: We find that replacing the training objective with DSC introdu", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The data used in this work is from the BIBREF0 paper.\n\nQuestion: What is the neural network architecture?\n\nAnswer: The neural network architecture is a bi-LSTM encoder with a linear decoder.\n\nQuestion: What is the training objective?\n\nAnswer: The training objective is to minimize the negative log-likelihood of the data.\n\nQuestion: What is the training data?\n\nAnswer: The training data is the data from the BIBREF0 paper.\n\nQuestion: What is the training procedure?\n\nAnswer: The training procedure is to train", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "The subjects were presented with a series of 100-ms visual stimuli consisting of 100-ms-long, 100-Hz square-wave gratings.\n\nQuestion: What was the stimulus presentation rate?\n\nAnswer: The stimulus presentation rate was 100 Hz.\n\nQuestion: What was the stimulus duration?\n\nAnswer: The stimulus duration was 100 ms.\n\nQuestion: What was the stimulus onset asynchrony (SOA)?\n\nAnswer: The stimulus onset asynchrony (SOA", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "We compare our model with the following baselines:\n\n1. Pointer-Gen: A pointer-generator model trained on the Weibo corpus.\n\n2. Pointer-Gen+Same-FT: A pointer-generator model trained on the Weibo corpus and the same training data as the pointer-generator model.\n\n3. Pointer-Gen+Pos-FT: A pointer-generator model trained on the same data as the pointer-generator model.\n\n4. Pointer-Gen+ARL-SEN: A pointer-generator model trained on the same data as", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "The dataset contains 100K tweets with cross-validated labels. We investigate the efficacy of different learning models in detecting abusive language. We compare accuracy using the most frequently studied machine learning classifiers as well as recent neural network models. Reliable baseline results are presented with the first comparative study on this dataset. Additionally, we demonstrate the effect of different features and variants, and describe the possibility for further improvements with the use of ensemble models.\n\nQuestion: What are the results of the learning models?\n\nAnswer: As traditional machine learning classifiers, Logistic Regression and Support", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "We use the Big Transformer architecture (BIBREF16 ) for all experiments.\n\nQuestion: What is the vocabulary size of the language model?\n\nAnswer: The vocabulary size of the language model is 37K.\n\nQuestion: What is the vocabulary size of the sequence to sequence model?\n\nAnswer: The vocabulary size of the sequence to sequence model is 37K.\n\nQuestion: What is the number of parameters of the language model?\n\nAnswer: The number of parameters of the language model is 190", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "Weights are dynamically adjusted based on the number of examples in each class.\n\nQuestion: What is the difference between dice loss and Tversky index?\n\nAnswer: Dice loss is a hard version of F1 score, while Tversky index is a soft version of F1 score.\n\nQuestion: What is the difference between dice loss and Tversky index?\n\nAnswer: Dice loss is a hard version of F1 score, while Tversky index is a soft version of F1 score.\n\nQuestion: What is the difference between dice loss and Tversky index?\n", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "The proposed strategies are able to pass the bottleneck in Zork1.\n\nAnswer: The proposed strategies are able to pass the bottleneck in Zork1.\n\nAnswer: The proposed strategies are able to pass the bottleneck in Zork1.\n\nAnswer: The proposed strategies are able to pass the bottleneck in Zork1.\n\nAnswer: The proposed strategies are able to pass the bottleneck in Zork1.\n\nAnswer: The proposed strategies are able to pass the bottleneck in Zork1.", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model consists of a set of parameters that are learned from the data.\n\nQuestion: What is the monolingual model?\n\nAnswer: The monolingual model is a model that is trained on a single language.\n\nQuestion: What is the multilingual model?\n\nAnswer: The multilingual model is a model that is trained on multiple languages.\n\nQuestion: What is the crosslingual latent variable?\n\nAnswer: The crosslingual latent variable is a variable that is used to capture crosslingual information.\n\nQuestion: What is the crossling", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "Non-standard pronunciation is identified by comparing the transcription to the audio.\n\nQuestion: How is non-standard pronunciation corrected?\n\nAnswer: Non-standard pronunciation is corrected by replacing the non-standard pronunciation with the non-standard pronunciation.\n\nQuestion: How is non-standard pronunciation corrected?\n\nAnswer: Non-standard pronunciation is corrected by replacing the non-standard pronunciation with the non-standard pronunciation.\n\nQuestion: How is non-standard pronunciation corrected?\n\nAnswer: Non-standard pronunci", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semicharacter architecture is a type of neural network that processes input words as a sequence of characters, rather than as a sequence of words.\n\nQuestion: What is the purpose of adversarial machine learning?\n\nAnswer: The purpose of adversarial machine learning is to improve the performance of a machine learning model on a specific task.\n\nQuestion: What is the purpose of adversarial attacks?\n\nAnswer: The purpose of adversarial attacks is to improve the performance of a machine learning model on a specific task.\n\nQuestion: What is the purpose of adversarial attacks?\n\nAnswer: The purpose of", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish.\n\nQuestion: what is the motivation for the choice of languages?\n\nAnswer: The motivation is that the languages are typologically, morphologically, and syntactically diverse.\n\nQuestion: what is the motivation for the choice of the feature-based models?\n\nAnswer: The motivation is that the choice of the feature-based models is that the feature-based models are more suitable for", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "NCEL is effective overall.\n\nQuestion: How does NCEL compare to other collective entity linking approaches?\n\nAnswer: NCEL outperforms other collective entity linking approaches.\n\nQuestion: How does NCEL compare to other local entity linking approaches?\n\nAnswer: NCEL outperforms other local entity linking approaches.\n\nQuestion: How does NCEL compare to other neural entity linking approaches?\n\nAnswer: NCEL outperforms other neural entity linking approaches.\n\nQuestion: How does NCEL compare to other neural entity linking approaches?\n\nAnswer", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: No\n\nQuestion: Is the data de-identified?\n\nAnswer: No\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: No\n\nQuestion: Is the data de-identified?\n\nAnswer: No\n\nQuestion: Is the data de-identified?\n\nAnswer: No\n\nQuestion: Is the data de-identified?\n\nAnswer: No\n\nQuestion: Is the data de", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The baseline used was the error detection system by Rei2016, trained on the same FCE dataset.\n\nQuestion: What is the difference between the two error generation methods?\n\nAnswer: The two error generation methods are different in the way they generate errors. The pattern-based method uses a set of patterns to insert errors into the input text, while the machine translation method translates from correct to incorrect sentences.\n\nQuestion: How did you evaluate the performance of the error detection models?\n\nAnswer: The performance of the error detection models was evaluated using the INLINEFORM0 metric, which is", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "The annotated clinical notes were obtained from the i2b2 database.\n\nQuestion: what is the name of the database?\n\nAnswer: The name of the database is i2b2.\n\nQuestion: what is the name of the task?\n\nAnswer: The name of the task is entity tagging.\n\nQuestion: what is the name of the task?\n\nAnswer: The name of the task is entity tagging.\n\nQuestion: what is the name of the task?\n\nAnswer: The name of the task is entity tagging.\n\nQuestion: what is the", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder is helpful because it forces the model to focus on the most important words in the input sequence. By masking out words, the model is forced to generate the most likely word given the input sequence representation vector and previous decoded output.\n\nQuestion: Why do you use the two-stage decoder?\n\nAnswer: We use the two-stage decoder because it helps the model to generate more diverse summaries.\n\nQuestion: Why do you use the refine objective?\n\nAnswer: We use the refine objective because it helps the model to generate more diverse summaries", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "The authors use the Twitter Firehose dataset, which is a publicly available dataset containing 100 million tweets.\n\nQuestion: What is the objective function they optimize?\n\nAnswer: The objective function they optimize is to predict the next word in a tweet.\n\nQuestion: What is the architecture of the model?\n\nAnswer: The model is a recurrent neural network with a bidirectional LSTM layer.\n\nQuestion: What is the training procedure?\n\nAnswer: The training procedure involves training the model on the dataset and then evaluating the model on a held-out dataset", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "The features used are TF-IDF features.\n\nQuestion: What is the performance of the model?\n\nAnswer: The performance of the model is 92% accuracy.\n\nQuestion: What is the performance of the model?\n\nAnswer: The performance of the model is 92% accuracy.\n\nQuestion: What is the performance of the model?\n\nAnswer: The performance of the model is 92% accuracy.\n\nQuestion: What is the performance of the model?\n\nAnswer: The performance of the model is 92% accuracy.\n\nQuestion: What", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated with 9,473 annotations for 9,300 tweets. Each tweet is annotated as no evidence of depression (e.g., “Citizens fear an economic depression\") or evidence of depression (e.g., “depressed over disappointment\"). If a tweet is annotated evidence of depression, then it is further annotated with one or more depressive symptoms, for example, depressed mood (e.g., “feeling down in the dumps\") disturbed sleep (e.g., “another rest", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated by using the machine translation platform Apertium.\n\nQuestion: How was the data augmented?\n\nAnswer: The training data was augmented by translating the English datasets into Spanish.\n\nQuestion: How were the models trained?\n\nAnswer: The models were trained using the Keras library.\n\nQuestion: How were the models ensembles created?\n\nAnswer: The models were ensembles created by averaging the predictions of the individual models.\n\nQuestion: How were the models ensembles created?\n\nAnswer: The models were ensembles created by", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "They used a content-based classifier.\n\nQuestion: What is the name of the company that they are working for?\n\nAnswer: They are working for Google.\n\nQuestion: What is the name of the company that they are working for?\n\nAnswer: They are working for Google.\n\nQuestion: What is the name of the company that they are working for?\n\nAnswer: They are working for Google.\n\nQuestion: What is the name of the company that they are working for?\n\nAnswer: They are working for Google.\n\nQuestion: What is the name of the company", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline was a simple bag-of-words model trained on the training set.\n\nQuestion: What is the difference between the sentence-level classification and the fragment-level classification?\n\nAnswer: The sentence-level classification is based on the whole sentences, while the fragment-level classification is based on the fragments.\n\nQuestion: What is the difference between the sentence-level classification and the fragment-level classification?\n\nAnswer: The sentence-level classification is based on the whole sentences, while the fragment-level classification is based on the fragments.\n\nQuestion: What is the difference between the sentence-", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "The authors compare their model with the following baselines:\n\n1. A CRF model that does not use any pun detection information.\n\n2. A CRF model that uses the pun detection information to predict the pun location.\n\n3. A CRF model that uses the pun detection information to predict the pun location.\n\n4. A CRF model that uses the pun detection information to predict the pun location.\n\n5. A CRF model that uses the pun detection information to predict the pun location.\n\n6. A CRF model that uses the pun detection information to predict the pun location.", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "We use the same source-based approach as in BIBREF18, which is a consolidated strategy also adopted by BIBREF6BIBREF16BIBREF1BIBREF2BIBREF3BIBREF11BIBREF12BIBREF13BIBREF14BIBREF15BIBREF16BIBREF17BIBREF18BIBREF19BIBREF20BIBREF21BIBREF22BIBREF23BIBREF24BIBREF25BIBREF26B", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of which are designed for Ancient-Modern Chinese alignment.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains INLINEFORM0 1.24M bilingual sentence pairs.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains INLINEFORM0 1.24M bilingual sentence pairs.\n\nQuestion: What is the size of the", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English\n\nQuestion: Are the tweets offensive?\n\nAnswer: Yes\n\nQuestion: Are the tweets insults?\n\nAnswer: Yes\n\nQuestion: Are the tweets threats?\n\nAnswer: Yes\n\nQuestion: Are the tweets profanity?\n\nAnswer: Yes\n\nQuestion: Are the tweets targeted at an individual?\n\nAnswer: Yes\n\nQuestion: Are the tweets targeted at a group?\n\nAnswer: Yes\n\nQuestion: Are the tweets targeted at other?\n\nAnswer: Yes\n\nQuestion: Are", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "The UTCNN model has 3 layers.\n\nQuestion: What is the name of the model that combines user, topic, and comment information?\n\nAnswer: The name of the model that combines user, topic, and comment information is UTCNN.\n\nQuestion: What is the name of the model that combines user, topic, and comment information?\n\nAnswer: The name of the model that combines user, topic, and comment information is UTCNN.\n\nQuestion: What is the name of the model that combines user, topic, and comment information?\n\nAnswer: The name of", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is the European network of nature protected sites Natura 2000 dataset.\n\nQuestion: what is the purpose of this paper?\n\nAnswer: The purpose of this paper is to show how to use Flickr tags to predict the distribution of 100 species across Europe.\n\nQuestion: what is the main hypothesis?\n\nAnswer: The main hypothesis is that by using vector space embeddings instead of bag-of-words representations learned from Flickr tags.\n\nQuestion: what is the use of vector space embeddings instead of bag-of-", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The datasets used in the paper are:\n\n1. MEDDOCAN: Medical Document Anonymization shared task dataset BIBREF3\n2. NUBes: A corpus of real medical reports in Spanish BIBREF4\n\n\nQuestion: What is the objective of the anonymisation task?\n\nAnswer: The objective of the anonymisation task is to anonymise the data containing sensitive information.\n\nQuestion: What are the systems and algorithms evaluated in the paper?\n\nAnswer: The systems evaluated in the paper are:\n\n1. BERT-based sequence labelling approach\n", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "They used unigram features, sarcasm features, readability features, and word count features.\n\nQuestion: What is the name of the eye-tracker they used?\n\nAnswer: They used an SR-Research Eyelink-10000 eye-tracker.\n\nQuestion: What is the name of the database they used?\n\nAnswer: They used a database of 1,00000000000000000000000000000000000000", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are as follows:\n\n1. The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are as follows:\n\n2. The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are as follows:\n\n3. The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are as follows:\n\n4. The metrics used to establish that this makes chatbots more knowledgeable and better", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Yes\n\nAnswer: No\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahçe\n\nQuestion: What is the stance of the author of the text for Galatasaray?\n\nAnswer: Favor\n\nQuestion: What is the stance of the author of the text for Fenerbahçe?\n\nAnswer: Against\n\nQuestion: What is the stance of the author of the text for Galatasaray?\n\nAnswer: Against\n\nQuestion: What is the stance of the author of the text for Fenerbahçe?\n\nAnswer: Favor\n\nQuestion: What", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "The experiments are conducted on the dataset crawled from twitter.\n\nQuestion: What is the dataset?\n\nAnswer: The dataset is crawled from twitter.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The size of the dataset is 262,7555,262,755,262,755,262,755,262,755,262,755,262,755,262,755,262,755", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention which is used to capture the localness and directional information of input sequence. It is a kind of self-attention network which is composed of one self-attention layer and a position-wise feed-forward layer.\n\nQuestion: What is the Transformer?\n\nAnswer: The Transformer is a kind of self-attention network which is composed of one self-attention layer and a position-wise feed-forward layer. The Transformer uses residual connections around the sublayers and then followed by a layer normal", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "Twitter\n\nQuestion: What is the main point of the article?\n\nAnswer: The article is about the use of causal explanations in social media.\n\nQuestion: What is the main point of the article?\n\nAnswer: The article is about the use of causal explanations in social media.\n\nQuestion: What is the main point of the article?\n\nAnswer: The article is about the use of causal explanations in social media.\n\nQuestion: What is the main point of the article?\n\nAnswer: The article is about the use of causal explanations in social media", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The baseline features are the features that are extracted from the network.\n\nQuestion: What are the network's pre-trained features?\n\nAnswer: The pre-trained features are the features that are extracted from the pre-trained models.\n\nQuestion: What is the difference between the baseline features and the pre-trained features?\n\nAnswer: The difference between the baseline features and the pre-trained features is that the baseline features are the features that are the pre-trained features.\n\nQuestion: What is the difference between the baseline features and the pre-", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "The hyperparameters varied in the experiments on the four tasks are: (i) the number of clusters, (ii) the number of classes, (iii) the number of words in the vocabulary, (iv) the number of iterations of the k-means algorithm.\n\n\nQuestion: What is the number of clusters used in the experiments on the four tasks?\n\nAnswer: The number of clusters used in the experiments on the four tasks are: (i) 2500, (ii) 100, (iii) 200, (iv) 500", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The scores of their system were 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus is 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average.\n\nQuestion: What is the average length of the entities?\n\nAnswer: The numbers of annotated entities are summarized in Table TABREF24.\n\nQuestion: What is the most frequently annotated type of entity?\n\nAnswer: Findings are the most frequently annotated type of entity.\n\nQuestion: What is the most frequently annotated type of entity?\n\nAnswer: Findings are the most frequently annot", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes, it is possible to convert a cloze-style questions to a naturally-looking questions.\n\nQuestion: What is the best way to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer: The best way to convert a cloze-style questions to a naturally-looking questions is to use a language modeling to generate a question.\n\nQuestion: What is the best way to convert a cloze-style questions to a naturally-looking questions?\n\nAnswer: The best way to convert a cloze-style questions to a naturally-looking questions is to use a language model", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "We possess a wealth of prior knowledge about many natural language processing tasks. For example, in text categorization, we know that words such as player, basketball, and football are strong indicators of the sports category.\n\nQuestion: What is the problem?\n\nAnswer: How to leverage such knowledge to guide the learning process, an interesting problem for both NLP and machine learning communities.\n\nQuestion: What is the problem?\n\nAnswer: How to leverage prior knowledge to the model for the performance may be a crucial problem, which has rarely been addressed, would the model be a baseball fan but unfamil", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "The model is compared to the following methods:\n\n1. The model is compared to the following methods:\n\n2. The model is compared to the following methods:\n\n3. The model is compared to the following methods:\n\n\n4. The model is compared to the following methods:\n\n5. The model is compared to the following methods:\n\n6. The model is compared to the following methods:\n\n\n7. The model is compared to the following methods:\n\n\n8. The model is compared to the following methods:\n\n\n9. The model is compared to the following", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The training sets of these versions of ELMo are larger than the previous ones.\n\nQuestion: What is the size of the training sets of these versions of ELMo?\n\nAnswer: The size of the training sets of these versions of ELMo are 20 million words.\n\nQuestion: What is the size of the training sets of the previous versions of ELMo?\n\nAnswer: The size of the training sets of the previous versions of ELMo are 20 million words.\n\nQuestion: What is the size of the training sets of the previous versions of ELMo?\n\n\nAnswer: The", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "1000\n\nQuestion: What is the average length of a sentence in the dataset?\n\nAnswer: 10\n\nQuestion: What is the average number of entities per sentence in the dataset?\n\nAnswer: 1.5\n\nQuestion: What is the average number of entities per sentence in the dataset?\n\nAnswer: 1.5\n\nQuestion: What is the average number of entities per sentence in the dataset?\n\nAnswer: 1.5\n\nQuestion: What is the average number of entities per sentence in the dataset?\n\nAnswer: 1.5", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "They compare to the following models/frameworks:\n\n1. MFCC\n2. MFCC + SVM\n3. MFCC + SVM + LDA\n4. MFCC + SVM + LDA + SVM + LDA + SVM + LDA + SVM + LDA + SVM + LDA + SVM + LDA + SVM + LDA + SVM + LDA + SVM + LDA + SVM + LDA + SVM + LDA + SVM + LDA + SVM + LDA + SVM + LDA", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes.\n\nQuestion: How does their model learn NER from both text and images?\n\nAnswer: The model learns NER from both text and images by combining word and character embeddings from both text and image.\n\nQuestion: What is the main result of their model?\n\nAnswer: The main result of their model is that the model performs better when visual information is available.\n\nQuestion: What is the main result of their model?\n\nAnswer: The main result of their model is that the model performs better when visual information is available.\n\nQuestion: What is the main result of", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "0.6103\n\nQuestion: What was their highest ‘MRR’ score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest ‘MRR’ score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest ‘MRR’ score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest ‘MRR’ score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest ‘MRR’ score?\n\nAnswer: 0.", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "They evaluate on sections 02-21 of WSJ corpus.\n\nQuestion: What is the length of the sentences they use?\n\nAnswer: The length of the sentences they use is 100.\n\nQuestion: What is the length of the sentences they use?\n\nAnswer: The length of the sentences they use is 100.\n\nQuestion: What is the length of the sentences they use?\n\nAnswer: The length of the sentences they use is 100.\n\nQuestion: What is the length of the sentences they use?\n\n\nAnswer", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques by conducting a survey among engineers and identifying a spectrum of three typical personas.\n\nQuestion: What are the three typical personas?\n\nAnswer: The three typical personas are:\n\n1. Engineers who are familiar with the frameworks and models, and are willing to spend time to learn new frameworks and models.\n\n2. Engineers who are familiar with the frameworks and models, but are not willing to spend time to learn new frameworks and models.\n\n3. Engineers who are", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "We have evaluated our system on two benchmarks: (1) SimpleQuestions BIBREF2 , which contains 14,000 questions and 100,000 relations, and (2) WebQuestions BIBREF0 , which contains 100,0000 questions and 100,000 relations.\n\nQuestion: What is the relation detection model?\n\nAnswer: We use a BiLSTM BiLSTM BiLSTM BiLSTM BiLSTM BiLSTM BiLSTM BiLSTM Bi", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
