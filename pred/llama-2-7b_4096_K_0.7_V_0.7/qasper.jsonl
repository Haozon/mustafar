{"pred": "The ground truth for fake news is established by a team of experts who manually labelled the tweets in the dataset.\n\nQuestion: What is the dataset used in this study?\n\nAnswer: The dataset used in this study is the one published by the authors of the paper.\n\nQuestion: What is the dataset used in this study?\nAnswer: The dataset used in this study is the one published by the authors of the paper.\n\nQuestion: What is the dataset used in this study?\nAnswer: The dataset used in this study is the one published by the authors of the paper.\n\nQuestion", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "GhostVLAD is an extension of the NetVLAD approach where we add Ghost clusters along with the NetVLAD clusters. So, now we will have a K+G number of clusters instead of K clusters. Where G is the number of ghost clusters, we want to add (typically 2-4). The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage, as shown in Figure 1 (Right side). Which means that we compute the matrix V for both normal cluster K and ghost clusters G, but we will", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "The proposed model outperforms the state of the art results by 68.8% to 71.8%.\n\nQuestion: What is the name of the dataset used in the paper?\n\nAnswer: The dataset used in the paper is the IEMOCAP dataset.\n\nQuestion: What is the name of the model?\n\nAnswer: The name of the model is the multimodal dual recurrent encoder (MDRE).\n\nQuestion: What is the name of the model?\n\nAnswer: The name of the model is the multimodal dual recurrent encoder", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "The use of context tweets generally have little effect on baseline models, however they noticeably improve the scores of several metrics. For instance, CNN with context tweets score the highest recall and F1 for “hateful\" labels, and RNN models with context tweets have the highest recall for “abusive\" tweets.\n\nAnswer:\n\n### 1. What is the dataset used in this paper?\n\nAnswer:\n\n### 2. What is the dataset used in this paper?\n\nAnswer:\n\n### 3. What is the dataset used in this paper", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They looked at Facebook pages for the following domains:\n\n\\begin{itemize}\n\\item FoxNews\n\\item CNN\n\\item ESPN\n\\item New York Times\n\\item Time magazine\n\\item Huffington Post\n\\item Weird News\n\\item The Guardian\n\\item Cartoon Network\n\\item Cooking Light\n\\item Home Cooking Adventure\n\\item Nickelodeon\n\\item Spongebob\n\\item Disney\n\\end{itemize}\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\nAnswer:\n", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "No, the hashtag dataset contains both English and non-English data.\n\nQuestion: How many hashtags are in the hashtag dataset?\n\nAnswer: The hashtag dataset contains 12,594 hashtags.\n\nQuestion: How many hashtags are in the hashtag dataset?\n\nAnswer: The hashtag dataset contains 12,594 hashtags.\n\nQuestion: How many hashtags are in the hashtag dataset?\n\nAnswer: The hashtag dataset contains 12,594 hasht", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "The task is defined as the creation of a concept map from a set of documents.\n\nQuestion: What is the goal of the proposed task?\n\nAnswer: The goal of the proposed task is to create a concept map that represents the most important content of a set of documents.\n\nQuestion: What is the corpus?\n\nAnswer: The corpus is a collection of documents that are used to train a model for the task.\n\nQuestion: What is the corpus size?\nAnswer: The corpus size is 30 topics, each of them contains around 40 documents.\n\nQuestion", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "We used the CNN/DailyMail dataset for extractive evaluation and the XSum dataset for abstractive evaluation.\n\nQuestion: What is the difference between extractive and abstractive summarization?\n\nAnswer: Extractive summarization is a task where the goal is to produce a summary that is a subset of the original document. Abstractive summarization is a task where the goal is to produce a summary that is not a subset of the original document.\n\nQuestion: What is the difference between extractive and abstractive summarization?\n\nAnswer: Extractive summarization is a task where the goal is to", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The proposed approach is compared with other WSD approaches employing word embeddings. The proposed approach is compared with other WSD approaches employing word embeddings.\n\nAnswer: The proposed approach is compared with other WSD approaches employing word embeddings.\n\nAnswer: The proposed approach is compared with other WSD approaches employing word embeddings.\n\nAnswer: The proposed approach is compared with other WSD approaches employing word embeddings.\n\nAnswer: The proposed approach is compared with other WSD approaches employing word embeddings.\n\nAnswer: The proposed approach is compared with", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The ensemble method works by taking a large number of small models and combining them into a single large model. This allows the ensemble to take advantage of the fact that each individual model may have its own strengths and weaknesses, and by combining them together, the ensemble can be more accurate than any individual model.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The two datasets are different in that the CNN/Daily Mail contains a lot of text from the news articles, while the BookTest contains a lot of children's books.\n\nQuestion: What is the difference between the two datasets", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The dataset is from the Friends TV sitcom.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is from the Friends TV sitcom.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is from the Friends TV sitcom.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is from the Friends TV sitcom.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is from the Friends TV sitcom.\n\nQuestion: What is the size of", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English\n\nAnswer: what language does this paper focus on?\n\nAnswer: English\n\nAnswer: what language does this paper focus on?\n\nAnswer: English\n\nAnswer: what language does this paper focus on?\n\nAnswer: English\n\nAnswer: what language does this paper focus on?\n\nAnswer: English\n\nAnswer: what language does this paper focus on?\n\nAnswer: English\n\nAnswer: what language does this paper focus on?\n\nAnswer: English\n\nAnswer: what language does this paper focus on?\n\nAnswer: English\n\nAnswer: what language does this", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset\n\nQuestion: What is the name of the NER dataset?\n\nAnswer: Groningen Meaning Bank (GMB)\n\nQuestion: What is the name of the corpus used for the analogy test?\n\nAnswer: Google Analogy Test\n\nQuestion: What is the name of the corpus used for the WordSimilarity test?\n\nAnswer: WordSimilarity-353\n\nQuestion: What is the name of the corpus used for the NER task?\n\nAnswer: Groningen Meaning Bank (GMB)\n\nQuestion: What", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The proposed system achieves an accuracy of 99.99%.\n\nQuestion: What is the main difference between the proposed system and the baseline system?\n\nAnswer: The proposed system uses adversarial training to learn worker-independent features, while the baseline system does not.\n\nQuestion: What is the main difference between the proposed system and the baseline system?\n\nAnswer: The proposed system uses adversarial training to learn worker-independent features, while the baseline system does not.\n\nQuestion: What is the main difference between the proposed system and the baseline system?\n\n", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes\n\nAnswer: No No No\n\nAnswer: Unanswerable Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable Unanswerable Unanswerable Unanswerable\n\nAnswer: Unanswerable Un", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "The dataset used in this paper is the 2015-2016 data from the National Health Interview Survey (NHIS) [1].\n\nQuestion: What is the dataset used?\n\nAnswer: The dataset used is the 2015-2016 data from the National Health Interview Survey (NHIS).\n\nQuestion: What is the dataset used?\n\nAnswer: The dataset used is the 2015-2016 data from the National Health Interview Survey (NHIS).\n\nQuestion: What is the dataset used?", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The Health Care sector achieved the best performance.\n\nQuestion: Which stock market sector achieved the worst performance?\n\nAnswer: The Energy sector achieved the worst performance.\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer: The Health Care sector achieved the best performance.\n\nQuestion: Which stock market sector achieved the worst performance?\n\nAnswer: The Energy sector achieved the worst performance.\n\nQuestion: Which stock market sector achieved the best performance?\n\nAnswer: The Health Care sector achieved the best performance.\n\nQuestion: Which stock market sector achieved the worst performance?\nAnswer: The", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "RNN-based NMT, Transformer-based NMT, and SMT.\n\nQuestion: what is the name of the dataset?\n\nAnswer: ancient-modern Chinese dataset.\n\nQuestion: what is the name of the proposed method?\n\nAnswer: clause alignment.\n\nQuestion: what is the proposed method based on?\n\nAnswer: the proposed method is based on the characteristics of the ancient-modern Chinese.\n\nQuestion: what is the proposed method is used for?\n\nAnswer: the proposed method is used for creating a large translation parallel corpus.\n\nQuestion:", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are neutral features, maximum entropy, and KL divergence.\n\nQuestion: What is the purpose of the neutral features?\n\nAnswer: The purpose of the neutral features is to prevent the model from being biased to a specific class.\n\nQuestion: What is the purpose of the maximum entropy regularization term?\n\nAnswer: The purpose of the maximum entropy regularization term is to control the distribution of the model's expectation.\n\nQuestion: What is the purpose of the KL divergence?\n\nAnswer: The purpose of the KL divergence is to control the model", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "The baselines are the following:\n\n1. SVM with n-gram feature\n2. SVM with word embedding feature\n3. CNN with word embedding feature\n4. RCNN with word embedding feature\n5. ILP with word embedding feature\n6. CRF with word embedding feature\n7. PSL with word embedding feature\n\nQuestion: What is the FBFans dataset?\n\nAnswer: The FBFans is a dataset of Facebook posts.\n\nQuestion: What is the CreateDebate dataset?\nAnswer: The CreateDebate is a dataset of CreateDebate.\n\nQuestion", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "By how much did they improve?\n\nAnswer: By how much did they improve?\n\nAnswer: By how much did they improve?\n\nAnswer: By how much did they improve?\n\nAnswer: By how much did they improve?\n\nAnswer: By how much did they improve?\n\nAnswer: By how much did they improve?\n\nAnswer: By how much did they improve?\n\nAnswer: By how much did they improve?\n\nAnswer: By how much did they improve?\n\nAnswer: By how much did they improve?\n\nAnswer: By how much did they improve?", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "The attention distribution of each head is predicted typically using the softmax normalizing transform. As a result, all context words have non-zero attention weights. Recent work on single attention architectures suggest that using sparse normalizing transforms in attention mechanisms such as sparsemax – which can yield exactly zero probabilities for irrelevant words. Recent work on single attention architectures suggest that, using sparse normalizing transforms such as sparsemax – which can yield exactly zero probabilities for irrelevant words.\n\nQuestion: How does their model learn to attend to a sparse set of words?\n\nAnswer: The attention", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "the baseline is the sentence-level translation.\n\nQuestion: what is the DocRepair model?\n\nAnswer: the DocRepair model is a monolingual sequence-to-sequence model that corrects inconsistencies between sentence-level translations of a context-agnostic MT system.\n\nQuestion: what is the training data?\n\nAnswer: the training data is the monolingual document-level data.\n\nQuestion: what is the test data?\nAnswer: the test data is the sentence-level data.\n\nQuestion: what is the training data?\nAnswer:", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pre-trained on the MT dataset.\n\nQuestion: What is the attention module pretrained on?\n\nAnswer: The attention module is pre-trained on the MT dataset.\n\nQuestion: What is the attention module pretrained on?\n\nAnswer: The attention module is pre-trained on the MT dataset.\n\nQuestion: What is the attention module pretrained on?\n\nAnswer: The attention module is pre-trained on the MT dataset.\n\nQuestion: What is the attention module pretrained on?\n\nAnswer:", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "The cognitive features, along with textual features used in best available sarcasm detectors, are used to train binary classifiers against given sarcasm labels.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to explain the cognitive features in sarcasm detection.\n\nQuestion: What is the purpose of the article?\nAnswer: The purpose of the article is to explain the cognitive features in sarcasm detection.\n\nQuestion: What is the purpose of the article?\nAnswer: The purpose of the article is to explain the cognitive features", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The encoder has a character-based encoder-decoder architecture.\n\nQuestion: What is the name of the auxiliary task?\n\nAnswer: The auxiliary task is the prediction of morpho-syntactic descriptions.\n\nQuestion: What is the name of the main task?\n\nAnswer: The main task is the prediction of the surface form of a word.\n\nQuestion: What is the name of the context window?\n\nAnswer: The context window is the entire available context.\n\nQuestion: What is the name of the auxiliary task?\n\nAnswer: The auxiliary", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes\n\nAnswer: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: Is WordNet useful for taxonomic reasoning for this task?\n\nAnswer: Is WordNet useful", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###\n\n###", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "The labels are:\n\n(1) No answer: The patient does not mention the symptom or attribute.\n\n(2) Yes answer: The patient mentions the symptom or attribute.\n\n(3) No answer: The patient does not mention the symptom or attribute.\n\n(4) Unanswerable: The patient does not mention the symptom or attribute.\n\n(5) No answer: The patient does not mention the symptom or attribute.\n\n\nAnswer:\n\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "The number of training examples needed to train the task-specific encoder depends on the task and the dataset. In general, the more data that is available, the better the model will perform.\n\nQuestion: How much data is needed to train the universal encoder?\n\nAnswer: The amount of data needed to train the universal encoder depends on the task and the dataset. In general, the more data that is available, the better the model will perform.\n\nQuestion: How much data is needed to train the universal encoder?\n\nAnswer: The amount of data needed to train the universal encoder depends on", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "The Transformer is evaluated on the WMT 2014 English-German and English-French translation tasks.\n\nQuestion: What is the Transformer architecture?\n\nAnswer: The Transformer is a neural network that uses a self-attention mechanism to model the relationship between the input and output sequences.\n\nQuestion: What is the Transformer architecture?\n\nAnswer: The Transformer is a neural network that uses a self-attention to model the relationship between the input and output sequences.\n\nQuestion: What is the Transformer architecture?\n\nAnswer: The Transformer is a neural", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The improvement in performance for Estonian in the NER task is 1.2%\n\nAnswer: The improvement in performance for Estonian in the NER task is 1.2%\n\nAnswer: The improvement in performance for Estonian in the NER task is 1.2%\n\nAnswer: The improvement in performance for Estonian in the NER task is 1.2%\n\nAnswer: The improvement in performance for Estonian in the NER task is 1.2%\n\nAnswer: The improvement in performance for Estonian in the NER task is 1.2%", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They are a team of researchers from different disciplines, including computer science, sociology, and linguistics.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to provide an overview of the current state of the field of computational text analysis, and to discuss the challenges and opportunities that arise when using computational text analysis to study social and cultural questions.\n\nQuestion: What is the main point of the article?\n\nAnswer: The main point of the article is that computational text analysis can be a useful tool for studying social and cultural questions.\n\nQuestion", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Yes.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is a public dataset, and the Weibo dataset is a self-collected dataset.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is a public dataset, and the Weibo dataset is a self-collected dataset.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is a public dataset, and the Weibo dataset is a self-collected dataset.", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The languages that are similar to each other are the languages that are similar to each other.\n\nAnswer: The languages that are similar to each other are the languages that are similar to each other.\n\nAnswer: The languages that are similar to each other are the languages that are similar to each other.\n\nAnswer: The languages that are similar to each other are the languages that are similar to each other.\n\nAnswer: The languages that are similar to each other are the languages that are similar to each other.\n\nAnswer: The languages that are similar to each other are the languages that are similar to each other.", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "They compared the performance of the LSTM models with the performance of the HMM-GMM models.\n\nQuestion: what is the difference between the two models?\n\nAnswer: The LSTM models are deep learning models, while the HMM-GMM models are conventional models.\n\nQuestion: what is the difference between the two models?\n\nAnswer: The LSTM models are deep learning models, while the HMM-GMM models are conventional models.\n\nQuestion: what is the difference between the two models?\n\nAnswer: The LSTM models are deep learning models, while", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\n", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human judgements were assembled by the authors of the paper.\n\nQuestion: What is the size of the corpus used for training the model?\n\nAnswer: The size of the corpus used for training the model is 197,792 sentences.\n\nQuestion: What is the size of the monolingual corpus?\n\nAnswer: The size of the monolingual corpus is 567,772 sentences.\n\nQuestion: What is the size of the monolingual corpus?\n\nAnswer: The size of the monol", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated by measuring the accuracy of the reconstruction of the target sentence.\n\nAnswer: The models are evaluated by measuring the accuracy of the reconstruction of the target sentence.\n\nAnswer: The models are evaluated by measuring the accuracy of the reconstruction of the target sentence.\n\nAnswer: The models are evaluated by measuring the accuracy of the reconstruction of the target sentence.\n\nAnswer: The models are evaluated by measuring the accuracy of the reconstruction of the target sentence.\n\nAnswer: The models are evaluated by measuring the accuracy of the reconstruction of the target sentence.", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "The most common evaluation metrics for classification tasks are accuracy, precision, recall, and F1 score. Accuracy is the percentage of correct predictions out of the total number of predictions. Precision is the percentage of correct predictions out of the total number of relevant predictions. Recall is the percentage of relevant predictions out of the total number of relevant predictions. F1 score is the harmonic mean of precision and recall.\n\nAnswer the question as concisely as you can, using a single phrase or sentence. Do not provide any explanation.\n\nAnswer:\n\nAnswer:\nAnswer:\nAnswer:\nAnswer:", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the domain from which the data is sampled, and the target domain is the domain to which the data is to be transferred.\n\nQuestion: What is the purpose of the proposed method?\n\nAnswer: The purpose of the proposed method is to bridge the source and target domains.\n\nQuestion: What is the proposed method based on?\n\nAnswer: The proposed method is based on the idea that domain-invariant feature representations should be aligned with the help of a key hypothesis that the key hypothesis that the key hypothesis that the key hypothesis that the key hypothesis that the key hypothesis that the key hypothesis that", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "LSTMs and similar recurrent units.\n\nQuestion: what is the difference between the LSTM and the PRU?\n\nAnswer: The PRU is a new recurrent unit that uses the pyramidal transformation to model contextual information.\n\nQuestion: what is the difference between the LSTM and the PRU?\n\nAnswer: The LSTM is a gating structure, which allows for more control over the information passed along the information.\n\nQuestion: what is the difference between the LSTM and the PRU?\n\nAnswer: The LSTM is a g", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "NeuronBlocks includes the following neural network modules:\n\n1. Word/character embedding\n2. CNN/RNN/Transformer\n3. Attention\n4. Dropout\n5. Layer Norm\n6. Batch Norm\n7. Focal Loss\n8. F1/Accuracy/Precision/Recall\n9. ExactMatch/F1\n10. MSE/RMSE\n11. AUC\n12. Accuracy\n13. Precision/Recall\n14. F1\n15. Ex", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "The authors used the Wiktionary corpus, which contains pronunciation data for 100,000 words in 100 languages.\n\nQuestion: what is the purpose of the paper?\n\nAnswer: The purpose of the paper is to leverage high resource language data to improve pronunciation data for low resource languages.\n\nQuestion: what is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is to leverage high resource data to improve pronunciation data for low resource languages.\n\nQuestion: what is the main contribution of the paper?\n", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines were:\n\n1. BERT\n2. XLNet\n3. RoBERTa\n\nQuestion: What was the dataset used for training?\n\nAnswer: The dataset used for training was:\n\n1. BioScope Abstracts\n2. BioScope Full Papers\n3. Sherlock\n4. SFU Review\n\nQuestion: What was the dataset used for testing?\n\nAnswer: The dataset used for testing was:\n\n1. BioScope Abstracts\n2. BioScope Full Papers\n3. Sherlock\n\nQuestion: What was the dataset used for", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "English\n\nQuestion: What is the name of the dataset they use?\n\nAnswer: XNLI\n\nQuestion: What is the name of the model they use?\n\nAnswer: XLM-R\n\nQuestion: What is the name of the model they use?\n\nAnswer: Roberta\n\nQuestion: What is the name of the model they use?\n\nAnswer: XLM-R\n\nQuestion: What is the name of the model they use?\n\nAnswer: BERT\n\nQuestion: What is the name of the model they use?\n\nAnswer: BERT\n\n", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "They test their method on the tasks of predicting hashtags for a held-out set of posts, and predicting the next word in a sentence.\n\nQuestion: What is the size of the training set?\n\nAnswer: The training set contains 2 million tweets.\n\nQuestion: What is the size of the test set?\n\nAnswer: The test set contains 50,000 tweets.\n\nQuestion: What is the size of the vocabulary?\n\nAnswer: The vocabulary contains 20,000 words.\n\nQuestion:", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes\n\nQuestion: Do they use pretrained embeddings?\n\nAnswer: Do they use pretrained embeddings?\n\nQuestion: Do they use pretrained embeddings?\n\nAnswer: Do they use pretrained embeddings?\n\nQuestion: Do they use pretrained embeddings?\n\nAnswer: Do they use pretrained embeddings?\n\nQuestion: Do they use pretrained embeddings?\n\nAnswer: Do they use pretrained embeddings?\n\nQuestion: Do they use pretrained embeddings?\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They obtain psychological dimensions of people by using the Meaning Extraction Method (MEM) BIBREF10 .\n\nAnswer: They obtain psychological dimensions of people by using the Meaning Extraction Method (MEM) BIBREF10 .\n\nAnswer: They obtain psychological dimensions of people by using the Meaning Extraction Method (MEM) BIBREF10 .\n\nAnswer: They obtain psychological dimensions of people by using the Meaning Extraction Method (MEM) BIBREF10 .\n\nAnswer: They obtain psychological dimensions of people by using the Meaning", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The ML methods aim to identify the following argument components:\n\n1. Claims\n2. Premises\n3. Rebuttals\n4. Backings\n5. Reasons\n6. Arguments\n7. Arguments in favor of the claim\n8. Arguments against the claim\n9. Arguments in favor of the rebuttal\n10. Arguments against the rebuttal\n11. Arguments in favor of the refutation\n12. Arguments against the refutation\n13. Arguments in favor of the conclusion\n14. Arguments against the conclusion", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, ", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "The Twitter dataset contains 100,000 tweets.\n\nQuestion: How large is the OSG dataset?\n\nAnswer: The OSG dataset contains 100,000,000 posts.\n\nQuestion: What is the sentiment of the original poster's final text?\n\nAnswer: The original poster's final text is positive.\n\nQuestion: What is the sentiment of the replying user's final text?\nAnswer: The replying user's final text is positive.\n\nQuestion: What is the sentiment of the replying user's", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "The 12 languages covered are English, French, German, Spanish, Italian, Portuguese, Russian, Polish, Czech, Hungarian, Romanian, and Serbian.\n\nQuestion: What is the purpose of the Multi-SimLex?\n\nAnswer: The purpose of the Multi-SimLex is to provide a large-scale, human-annotated, and consistent benchmark for evaluating multilingual word representations in NLP.\n\nQuestion: What is the purpose of the Multi-SimLex?\n\nAnswer: The purpose of the Multi-SimLex is to provide a large-scale", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "Wikipedia Gone Awry (CRAFT) and ChangeMyView.\n\nQuestion: What is the goal of the model?\n\nAnswer: To forecast the future derailment of a conversation.\n\nQuestion: What is the model's architecture?\n\nAnswer: A recurrent neural network that processes comments as they happen.\n\nQuestion: What is the model's training data?\n\nAnswer: Wikipedia talk page conversations.\n\nQuestion: What is the model's test data?\n\nAnswer: Crowd-sourced conversations from Reddit.\n\nQuestion: What", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "No.\n\nQuestion: Were any of the pipeline components based on deep learning models?\n\nAnswer: No.\n\nQuestion: Were any of the pipeline components based on deep learning models?\n\nAnswer: No.\n\nQuestion: Were any of the pipeline components based on deep learning models?\n\nAnswer: No.\n\nQuestion: Were any of the pipeline components based on deep learning models?\n\nAnswer: No.\n\nQuestion: Were any of the pipeline components based on deep learning models?\n\nAnswer: No.\n\nQuestion: Were any of the pipeline components", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The data is evaluated by comparing the performance of the model with the performance of a model trained on the same data but without the data. The performance is measured by the percentage of correct predictions.\n\nQuestion: What is the data source? \n\nAnswer: The data source is the Common Voice corpus.\n\nQuestion: What is the data type? \n\nAnswer: The data type is speech.\n\nQuestion: What is the data size? \nAnswer: The data size is 11,000,000,000,000,000,0", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They use a dual recurrent encoder, which combines the information from both the audio and text sequences.\n\nQuestion: What is the purpose of the attention mechanism in the proposed model?\n\nAnswer: The attention mechanism is used to improve the performance of the model by focusing on the most important information in the input.\n\nQuestion: What is the purpose of the proposed model?\n\nAnswer: The purpose of the proposed model is to improve the performance of the model by combining the information from both the audio and text sequences.\n\nQuestion: What is the purpose of the proposed model?\n\nAnswer: The", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by by", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "10\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 10\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 10\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 10\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 10\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 10\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 10\n\nQuestion: how many humans evaluated the results?\n\nAnswer: 1", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "Tweets going viral are those that are retweeted more than 1000 times.\n\nQuestion: What is the purpose of the study?\n\nAnswer: The purpose of the study is to understand the characteristics of viral tweets containing fake news.\n\nQuestion: What is the hypothesis?\n\nAnswer: The hypothesis is that there are specific pieces of meta-data about tweets that may allow the identification of fake news.\n\nQuestion: What is the first finding?\n\nAnswer: The first finding is that viral tweets containing fake news are shorter-lived than those containing other", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "CNN\n\nQuestion: Which basic neural architecture perform best when combined with BERT?\n\nAnswer: LSTM-CRF\n\nQuestion: Which basic neural architecture perform best when combined with BERT and CNN?\n\nAnswer: LSTM-CRF+Multi-grain\n\nQuestion: Which basic neural architecture perform best when combined with BERT and CNN?\n\nAnswer: LSTM-CRF+Multi-task\n\nQuestion: Which basic neural architecture perform best when combined with BERT and CNN?\n\nAnswer: LSTM-CRF+Multi-task\n\nQuestion:", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The DeepMine database was collected using crowdsourcing.\n\nQuestion: what is the size of the database?\n\nAnswer: The database consists of 1969 respondents, 1149 of them are male and 820 female.\n\nQuestion: what is the size of the database?\n\nAnswer: The database consists of 1969 respondents, 1149 of them are male and 820 female.\n\nQuestion: what is the size of the database?\n\nAnswer: The database consists of 1969 respondents", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "We use the following machine learning and deep learning methods for RQE:\n\n1. Logistic Regression\n2. Recurrent Neural Networks (RNNs)\n3. Long Short Term Memory cells (LSTMs)\n4. Convolutional Neural Networks (CNNs)\n\nQuestion: What is the effectiveness of the proposed approach?\n\nAnswer: The proposed approach is effective in the following ways:\n\n1. It is effective in the following ways:\n2. It is effective in the following ways:\n3. It is effective in the following ways:", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is the Honeypot dataset. It is a dataset of 2947 spammers and 2218 legitimate users. The quality of the dataset is high.\n\nQuestion: What is the dataset used in the paper?\n\nAnswer: The dataset used in the paper is the Honeypot dataset.\n\nQuestion: What is the dataset used in the paper?\n\nAnswer: The dataset used in the paper is the Honeypot dataset.\n\nQuestion: What is the dataset used in the paper?\n\nAnswer: The dataset used in the paper is", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder has a LSTM encoder and a LSTM decoder.\n\nQuestion: What is the name of the auxiliary task?\n\nAnswer: The auxiliary task is MSD prediction.\n\nQuestion: What is the name of the main task?\n\nAnswer: The main task is inflection.\n\nQuestion: What is the name of the dataset?\n\nAnswer: The dataset is the CoNLL–SIGMORPHON 2018 shared task on universal morphological reinflection.\n\nQuestion: What is the name of the shared task?\n\n", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No No.\n\nQuestion: Do they", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model among the authors' submissions is the ensemble of CNN, BERT and LSTM-CRF. It achieved an F1 score of 0.673 on the dev set.\n\nAnswer:\n\n### 1. What is the best performing model among the authors' submissions?\n\nThe best performing model among the authors' submissions is the ensemble of CNN, BERT and LSTM-CRF. It achieved an F1 score of 0.673 on the dev set.\n\n### 2. What is the best performing model among", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "the base model was the M2M Transformer model trained on the original parallel data.\n\nQuestion: what was the multilingual model?\n\nAnswer: the multilingual model was the M2M Transformer model trained on the multilingual data.\n\nQuestion: what was the multilingual data?\n\nAnswer: the multilingual data was the data from the other language pairs.\n\nQuestion: what was the multilingual model?\nAnswer: the multilingual model was the M2M Transformer model.\n\nQuestion: what was the multilingual model?\nAnswer", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "0.6103\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest F1 score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest precision score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest recall score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest F1 score?\nAnswer: 0.6103\n\nQuestion: What was their highest MR", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "The paper explores a number of different embedding techniques, including word2vec, GloVe, and FastText.\n\nQuestion: What is the purpose of the second–order vectors?\n\nAnswer: The purpose of the second–order vectors is to provide a measure of semantic relatedness between two terms.\n\nQuestion: What is the difference between the first and second order vectors?\n\nAnswer: The first order vectors are based on the co–occurrence of a term with itself, while the second order vectors are based on the co–occurrence of a term with another term.\n\nQuestion: What is", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "The pre-ordering system matches words based on their position in the sentence. For example, if the sentence is \"The boy hit the ball\", the pre-ordering system will match the word \"hit\" with the word \"ball\".\n\nAnswer: The pre-ordering system matches words based on their position in the sentence. For example, if the sentence is \"The boy hit the ball\", the pre-ordering system will match the words \"hit\" and \"ball\".\n\nAnswer: The pre-ordering system matches words based on their position in the sentence. For example, if the sentence is \"", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "Yes\n\nAnswer: No No\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "We used seven experts with legal training to construct answers to Turker questions. Experts identified relevant evidence within the privacy policy, as well as provide meta-annotation on the question's relevance, subjectivity, and how likely any privacy policy is to contain the answer to the question asked.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset consists of 1750 questions asked by 350 crowdworkers to 350 crowdworkers.\n\nQuestion: What is the distribution of questions?\nAnswer: The distribution of questions is 17", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "The painting embedding is done using a CNN-RNN generative model. The language style transfer is done using a seq2seq model with global attention.\n\nAnswer:\n\n### 2.\n\n### 3.\n\n### 4.\n\n### 5.\n\n### 6.\n\n### 7.\n\n### 8.\n\n### 9.\n\n### 10.\n\n### 11.\n\n### 12.\n\n### 13.\n\n### ", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer layer works better.", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes\n\nAnswer: No No\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed cyberbullying on social media.\n\nQuestion: What is the definition of cyberbullying?\n\nAnswer: The definition of cyberbullying is the use of the Internet, cell phones or other devices to send or post text or images that hurt or embarrass someone.\n\nQuestion: What is the definition of cyberbullying?\n\nAnswer: The definition of cyberbullying is the use of the Internet, cell phones or other devices to send or post text or images that hurt or embarrass someone.\n\nQuestion: What is the definition of cyberbullying?", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "There are 10 different types of entities in the dataset.\n\nQuestion: What is the most common type of entity in the dataset?\n\nAnswer: The most common type of entity in the dataset is \"PERSON\".\n\nQuestion: What is the most common type of entity in the dataset?\n\nAnswer: The most common type of entity in the dataset is \"PERSON\".\n\nQuestion: What is the most common type of entity in the dataset?\n\nAnswer: The most common type of entity in the dataset is \"PERSON\".\n\nQuestion: What is the most common type of entity in", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\nQuestion: How much higher quality is the resulting annotated data?\n\nAnswer: The resulting annotated data is of higher quality.\n\n", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "The gender imbalance in the analyzed corpora is 65% male speakers and 35% female speakers.\n\nQuestion: What is the gender imbalance in the analyzed corpora?\n\nAnswer: The gender imbalance in the analyzed corpora is 65% male speakers and 35% female speakers.\n\nQuestion: What is the gender imbalance in the analyzed corpora?\n\nAnswer: The gender imbalance in the analyzed corpora is 65% male speakers and 35% female speakers.", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "The approach achieves state of the art results on the English-German test set of the WMT 2016 and 2018 shared tasks.\n\nQuestion: What is the main difference between the deliberation models and the base models?\n\nAnswer: The deliberation models use a deliberation network to generate a second-pass translation. The second-pass translation is then used as the input to the standard transformer model.\n\nQuestion: What is the main difference between the deliberation models and the base models?\n\nAnswer: The deliberation models use a deliberation network to generate a", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "The Transformer is a neural network model for sequence-to-sequence learning proposed by Google in 2017. It is a sequence-to-sequence model that uses a self-attention mechanism to capture the long-range dependencies between input sequences.\n\nQuestion: What is the Transformer?\n\nAnswer: The Transformer is a neural network model for sequence-to-sequence learning proposed by Google in 2017. It is a sequence-to-sequence model that uses a self-attention mechanism to capture the long-range dependencies between input sequences.\n\nQuestion: What is the Trans", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "The classifiers used are deep neural networks.\n\nQuestion: What is the dataset used for training?\n\nAnswer: The dataset used for training is the Twitter dataset.\n\nQuestion: What is the dataset used for testing?\n\nAnswer: The dataset used for testing is the Twitter dataset.\n\nQuestion: What is the dataset used for validation?\n\nAnswer: The dataset used for validation is the Twitter dataset.\n\nQuestion: What is the dataset used for testing?\nAnswer: The dataset used for testing is the Twitter dataset.\n\nQuestion: What is the dataset used for training?\nAnswer: The", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "\n\n### References\n\n[BIBREF0]: https://www.nature.com/articles/s41598-018-32912-z\n\n[BIBREF1]: https://www.nature.com/articles/s41598-018-32912-z\n\n[BIBREF12]: https://pypi.org/project/senti-strength/\n\n[BIBREF13]: https://pypi.org/project/tensi-str", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "We performed experiments on the SQuAD dataset.\n\nQuestion: What is the size of the SQuAD dataset?\n\nAnswer: The SQuAD dataset contains 100,000 training examples and 10,000 testing examples.\n\nQuestion: What is the size of the SQuAD dataset?\n\nAnswer: The SQuAD dataset contains 100,000 training examples and 10,000 testing examples.\n\nQuestion: What is the size of the SQuAD dataset?\n\nAnswer: The SQuAD dataset contains ", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "\n\nQuestion: what is the proposed approach?\n\nAnswer:\n\nQuestion: what are the experimental results?\n\nAnswer:\n\nQuestion: what are the conclusions?\n\nAnswer:\n\nQuestion: what are the limitations?\n\nAnswer:\n\nQuestion: what are the future work?\n\nAnswer:\n\nQuestion: what are the conclusions?\n\nAnswer:\n\nQuestion: what are the limitations?\n\nAnswer:\n\nQuestion: what are the future work?\n\nAnswer:\n\nQuestion: what are the conclusions?\n\nAnswer:\n\nQuestion:", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes\n\nQuestion: Do they use attention?\n\nAnswer: No\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "They used 20newsgroups, Fisher, and CSAT datasets.\n\nQuestion: What is the difference between RoBERT and ToBERT?\n\nAnswer: RoBERT is a method that builds upon BERT's architecture. It splits the input sequence into segments in order to obtain a representation for each of them using BERT. Then, it uses either a recurrent LSTM or another Transformer to perform the actual classification.\n\nQuestion: What is the difference between RoBERT and ToBERT?\n\nAnswer: RoBERT is a method that builds upon BERT'", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "No.\n\nQuestion: Are the results of the BERT model comparable to the results of the LSTM model?\n\nAnswer: Yes.\n\nQuestion: Are the results of the BERT model comparable to the results of the Transformer model?\n\nAnswer: Yes.\n\nQuestion: Are the results of the BERT model comparable to the results of the LSTM model?\n\nAnswer: Yes.\n\nQuestion: Are the results of the BERT model comparable to the results of the Transformer model?\n\nAnswer: Yes.\n\nQuestion: Are the results of", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "No, the dataset is not balanced.\n\nAnswer: No, the dataset is not balanced.\n\nAnswer: No, the dataset is not balanced.\n\nAnswer: No, the dataset is not balanced.\n\nAnswer: No, the dataset is not balanced.\n\nAnswer: No, the dataset is not balanced.\n\nAnswer: No, the dataset is not balanced.\n\nAnswer: No, the dataset is not balanced.\n\nAnswer: No, the dataset is not balanced.\n\nAnswer: No, the dataset is not balanced.\n\n", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is that the projection function must be invertible.\n\nQuestion: What is the invertibility condition?\n\nAnswer: The invertibility condition is that the projection function must be invertible.\n\nQuestion: What is the invertibility condition?\n\nAnswer: The invertibility condition is that the projection function must be invertible.\n\nQuestion: What is the invertibility condition?\n\nAnswer: The invertibility condition is that the projection function must be invertible.\n\nQuestion: What is the invertibility condition?\n\nAnswer: The invertibility condition is that the projection function must be invert", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "\n\n### 1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "The size of the WikiSmall dataset is 89,042 sentence pairs, and the size of the WikiLarge dataset is 296,402 sentence pairs.\n\nQuestion: what are the results of the human evaluation?\n\nAnswer: The results of the human evaluation are shown in Table 1. We can see that our method (NMT+synthetic) can obtain higher BLEU, lower FKGL and high SARI compared with other models, except Dress on FKGL and SBMT-SARI on SARI.\n\nQuestion", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "The baselines are the following:\n\n1. A vanilla baseline, which is a cascaded system.\n\n2. A cascaded+re-seg baseline, which is a cascaded system with re-segmentation.\n\n3. A vanilla baseline, which is a cascaded system without re-segmentation.\n\n4. A vanilla+re-seg baseline, which is a cascaded system with re-segmentation.\n\n\n5. A vanilla+re-seg+pretrain baseline, which is a cascaded system", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English\n\nQuestion: What is the name of the dataset used in this paper?\n\nAnswer: PTC\n\nQuestion: What is the name of the task used in this paper?\n\nAnswer: Sentence-level classification\n\nQuestion: What is the name of the task used in this paper?\n\nAnswer: Fragment-level classification\n\nQuestion: What is the name of the task used in this paper?\n\nAnswer: Sentence-level classification\n\nQuestion: What is the name of the task used in this paper?\n\nAnswer: Fragment-level classification\n\nQuestion: What is the", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The models used in the experiment are a linear SVM, a BiLSTM, and a CNN.\n\nQuestion: What is the performance of the models?\n\nAnswer: The performance of the models is as follows:\n\nSVM:\n\nMacro-F1: 0.80\n\nPrecision: 0.80\n\nRecall: 0.80\n\nF1: 0.80\n\nBiLSTM:\n\nMacro-F1: 0.69\n\nPrecision: 0.69\n", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "No, they do not.\n\nQuestion: Do the answered questions measure for the quality of the answer?\n\nAnswer: No, they do not.\n\nQuestion: Do the answered questions measure for the relevance of the answer?\n\nAnswer: No, they do not.\n\nQuestion: Do the answered questions measure for the correctness of the answer?\n\nAnswer: No, they do not.\n\nQuestion: Do the answered questions measure for the timeliness of the answer?\n\nAnswer: No, they do not.\n\nQuestion: Do the answered questions measure for the quality of the", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "GloVe and Edinburgh embeddings.\n\nQuestion: what is the best performing model?\n\nAnswer: XGBoost with Edinburgh embeddings.\n\nQuestion: what is the best performing model for each emotion?\n\nAnswer: Edinburgh embeddings with XGBoost for anger, Edinburgh embeddings with AdaBoost for fear, Edinburgh embeddings with XGBoost for sadness, Edinburgh embeddings with AdaBoost for joy.\n\nQuestion: what is the best performing model for each emotion?\n\nAnswer: Edinburgh embeddings with XGBoost", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "The new dataset contains 180K recipes and 700K user reviews.\n\nQuestion: What is the name of the dataset?\n\nAnswer: Food.com\n\nQuestion: What is the name of the dataset?\n\nAnswer: Food.com\n\nQuestion: What is the name of the dataset?\n\nAnswer: Food.com\n\nQuestion: What is the name of the dataset?\n\nAnswer: Food.com\n\nQuestion: What is the name of the dataset?\n\nAnswer: Food.com\n\nQuestion: What is the name of the dataset?\n", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning is the sum of the rewards for content preservation, sentiment preservation, and irony accuracy.\n\nQuestion: What is the name of the model?\n\nAnswer: The name of the model is the combination of rewards for reinforcement learning and the combination of rewards for reinforcement learning.\n\nQuestion: What is the name of the model?\n\nAnswer: The name of the model is the combination of rewards for reinforcement learning.\n\nQuestion: What is the name of the model?\nAnswer: The name of the model", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate that their model does not work well when the style transfer dataset does not have similar words in the training set of sentences.\n\nAnswer: The authors demonstrate that their model does not work well when the style transfer dataset does not have similar words in the training set of sentences.\n\nAnswer: The authors demonstrate that their model does not work well when the style transfer dataset does not have similar words in the training set of sentences.\n\nAnswer: The authors demonstrate that their model does not work well when the style transfer dataset does not have similar words in the training set of sentences.\n\nAnswer: The authors demonstrate that", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "They compared to the Affective Text, Fairy Tales, and ISEAR datasets.\n\nQuestion: What is the Affective Text dataset?\n\nAnswer: The Affective Text dataset is a dataset of 1000 news headlines annotated with emotions and valence.\n\nQuestion: What is the Fairy Tales dataset?\n\nAnswer: The Fairy Tales dataset is a dataset of 1000 fairy tales annotated with emotions.\n\nQuestion: What is the ISEAR dataset?\n\nAnswer: The ISEAR dataset is a", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The distribution of the number of friends of the accounts that generated viral tweets containing fake news was different from the distribution of the number of friends of the accounts that generated viral tweets not containing fake news.\n\nQuestion: What were their distribution results?\n\nAnswer: The distribution of the number of friends of the accounts that generated viral tweets containing fake news was different from the distribution of the number of friends of the accounts that generated viral tweets not containing fake news.\n\nQuestion: What were their distribution results?\n\nAnswer: The distribution of the number of friends of the accounts that generated viral", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset is sourced from Twitter.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 12,594 unique hashtags and their associated tweets annotated in a multi-step process for higher quality than the previous dataset of 1,108 hashtags.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 12,594 unique hashtags and their associated tweets annotated in a multi-step process.\n\nQuestion: What is the size of the dataset?\n\n", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The corpus contains speech from speakers with different accents.\n\nQuestion: what is the average length of the utterances?\n\nAnswer: The average length of the utterances is 10 seconds.\n\nQuestion: what is the average length of the utterances?\n\nAnswer: The average length of the utterances is 10 seconds.\n\nQuestion: what is the average length of the utterances?\n\nAnswer: The average length of the utterances is 10 seconds.\n\nQuestion: what is the average length of the utterances?\n\nAnswer: The average length of", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace can represent the context of a text.\n\nQuestion: What is the main idea of the word subspace modeling?\n\nAnswer: The main idea of the word subspace modeling is to represent the context of a text as a low-dimensional vector.\n\nQuestion: What is the main idea of the word subspace modeling?\n\nAnswer: The main idea of the word subspace modeling is to represent the context of a text as a low-dimensional vector.\n\nQuestion: What is the main idea of the word subspace modeling?\n\nAnswer: The main idea of", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "Random Forest\n\nQuestion: What is the performance of the baseline model?\n\nAnswer: 0.12\n\nQuestion: What is the performance of the proposed model?\n\nAnswer: 0.66\n\nQuestion: What is the performance of the proposed model for the year 2010?\n\nAnswer: 0.66\n\nQuestion: What is the performance of the proposed model for the year 2011?\n\nAnswer: 0.66\n\nQuestion: What is the performance of the proposed model for the year 2012?", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "The dataset is 110 hours long.\n\nQuestion: How many languages are in the dataset?\n\nAnswer: The dataset contains 11 languages.\n\nQuestion: How many speakers are in the dataset?\n\nAnswer: The dataset contains 11,000 speakers.\n\nQuestion: How many sentences are in the dataset?\n\nAnswer: The dataset contains 110,000 sentences.\n\nQuestion: How many speakers are in the dataset?\n\nAnswer: The dataset contains 11,000 speakers.\n\nQuestion: How", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "The dataset used was the SemEval-2016 dataset.\n\nQuestion: What is the name of the dataset?\n\nAnswer: The dataset used was the SemEval-2016 dataset.\n\nQuestion: What is the name of the dataset?\n\nAnswer: The dataset used was the SemEval-2016 dataset.\n\nQuestion: What is the name of the dataset?\n\nAnswer: The dataset used was the SemEval-2016 dataset.\n\nQuestion: What is the name of the dataset?\n\nAnswer: The dataset used was", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\n", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes\n\nAnswer: The automatically constructed datasets are subject to quality control.\n\nAnswer: The automatically constructed datasets are subject to quality control.\n\nAnswer: The automatically constructed datasets are subject to quality control.\n\nAnswer: The automatically constructed datasets are subject to quality control.\n\nAnswer: The automatically constructed datasets are subject to quality control.\n\nAnswer: The automatically constructed datasets are subject to quality control.\n\nAnswer: The automatically constructed datasets are subject to quality control.\n\nAnswer: The automatically constructed datasets are subject to quality control.\n\nAnswer: The automatically constructed datasets are subject to quality control.\n", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "Yes\n\nAnswer: Are the images from a specific domain?\n\nAnswer: Are the images from a specific domain?\n\nAnswer: Are the images from a specific domain?\n\nAnswer: Are the images from a specific domain?\n\nAnswer: Are the images from a specific domain?\n\nAnswer: Are the images from a specific domain?\n\nAnswer: Are the images from a specific domain?\n\nAnswer: Are the images from a specific domain?\n\nAnswer: Are the images from a specific domain?\n\nAnswer: Are the images from a specific domain?\n\nAnswer: Are the images", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "The best model achieved an f-score of 0.368 on the development set.\n\nAnswer: The best model achieved an f-score of 0.368 on the development set.\n\nAnswer: The best model achieved an f-score of 0.368 on the development set.\n\nAnswer: The best model achieved an f-score of 0.368 on the development set.\n\nAnswer: The best model achieved an f-score of 0.368 on the development set.\n\nAnswer: The best model achieved an f-score", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The INLINEFORM0 tag is used to indicate that the word is not a pun. The INLINEFORM1 tag is used to indicate that the word is a pun. The INLINEFORM2 tag is used to indicate that the word is not a pun.\n\nQuestion: What is the tagging scheme employed?\n\nAnswer: The INLINEFORM0 tag is used to indicate that the word is not a pun. The INLINEFORM1 tag is used to indicate that the word is a pun. The INLINEFORM2 tag is used to indicate that the word is not a pun.\n\nQuestion: What is", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "No\n\nAnswer: Yes Yes\n\nAnswer: No No\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Un", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "Robustness is defined as the ability of a model to generalize to unseen data.\n\nQuestion: What is the difference between the proposed methods and the baseline method?\n\nAnswer: The proposed methods are more robust to the unbalanced data.\n\nQuestion: What is the difference between the proposed methods and the baseline method?\n\nAnswer: The proposed methods are more robust to the unbalanced data.\n\nQuestion: What is the difference between the proposed methods and the baseline method?\n\nAnswer: The proposed methods are more robust to the unbalanced data.\n\nQuestion:", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We We", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "For English datasets, our proposed method outperforms BERT-MRC by +0.29 and +0.96 respectively. For Chinese datasets, our proposed method outperforms BERT-MRC by +0.97 and +2.36 respectively.\n\nQuestion: What are the hyperparameters of dice loss?\n\nAnswer: The hyperparameters of dice loss are $\\alpha $ and $\\beta $.\n\nQuestion: What is the hyperparameters of Tversky index?\nAnswer: The hyperparameters of Tversky index are $\\alpha $ and $\\beta $.\n\nQuestion: What", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on two tasks: (1) pair-level classification and (2) ranking.\n\nQuestion: What is the conflict model?\n\nAnswer: The conflict model is a linear transformation of the input vectors that computes the difference between them.\n\nQuestion: What is the attention model?\n\nAnswer: The attention model is a linear transformation of the input vectors that computes the similarity between them.\n\nQuestion: What is the conflict model?\n\nAnswer: The conflict model is a linear transformation of the input vectors that computes the difference between them.\n\nQuestion: What is the", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "They compared against the following baselines:\n\n1. A bidirectional LSTM with a linear classifier\n\n2. A bidirectional LSTM with a softmax classifier\n\n3. A unidirectional LSTM with a softmax classifier\n\n4. A unidirectional LSTM with a linear classifier\n\n5. A bidirectional LSTM with a linear classifier\n\n6. A bidirectional LSTM with a softmax classifier\n\n7. A unidirectional LSTM with a linear classifier\n\n", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is the relation detection model.\n\nQuestion: What is the core component for KBQA?\n\nAnswer: The core component for KBQA is the relation detection model.\n\nQuestion: What is the core component for KBQA?\n\nAnswer: The core component for KBQA is the relation detection model.\n\nQuestion: What is the core component for KBQA?\n\nAnswer: The core component for KBQA is the relation detection model.\n\nQuestion: What is the core component for KBQA?", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are the encoder-decoder models that do not attend over user preferences.\n\nQuestion: What are the personalized models?\n\nAnswer: The personalized models are the encoder-decoder models that attend over user preferences.\n\nQuestion: What is the dataset?\n\nAnswer: The dataset is the Food.com dataset.\n\nQuestion: What is the dataset size?\n\nAnswer: The dataset size is 180K+.\n\nQuestion: What is the dataset split?\nAnswer: The dataset is split into training and test sets.\n\nQuestion", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "The methods to find examples of biases and unwarranted inferences are manual inspection of the data, and the use of machine learning techniques to identify patterns in the data.\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\n", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with the following models:\n\n1. Plain stacked LSTMs\n2. Models with different INLINEFORM0\n3. Models without INLINEFORM1\n4. Models that integrate lower contexts via peephole connections\n\nQuestion: What is the purpose of the proposed method?\n\nAnswer: The purpose of the proposed method is to enhance the expressiveness of RNNs by considering the vertical information flow.\n\nQuestion: What is the proposed method?\n\nAnswer: The proposed method is to enhance the expressiveness of RNNs by considering", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n\nAnswer: No No\n", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with several summarization algorithms, including the Sumy package.\n\nQuestion: What is the name of the algorithm used to generate the summaries?\n\nAnswer: The algorithm used to generate the summaries is the Sumy package.\n\nQuestion: What is the name of the algorithm used to generate the summaries?\n\nAnswer: The algorithm used to generate the summaries is the Sumy package.\n\nQuestion: What is the name of the algorithm used to generate the summaries?\n\nAnswer: The algorithm used to generate the summaries is the Sumy package.\n\nQuestion:", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "The previous state of the art for this task was to use a neural network to predict whether a post was an intervention or not.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is to propose a new model that can predict whether a post is an intervention or not.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer: The main contribution of this paper is to propose a new model that can predict whether a post is an intervention or not.\n\nQuestion: What is the main contribution of this paper?\n\nAnswer:", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The corpus used for the task is the DTA corpus.\n\nAnswer: The corpus used for the task is the DTA corpus.\n\nAnswer: The corpus used for the task is the DTA corpus.\n\nAnswer: The corpus used for the task is the DTA corpus.\n\nAnswer: The corpus used for the task is the DTA corpus.\n\nAnswer: The corpus used for the task is the DTA corpus.\n\nAnswer: The corpus used for the task is the DTA corpus.\n\nAnswer: The cor", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.\n\nQuestion: What is the dataset used for training?\n\nAnswer: The dataset is a collection of 635 hours of audio data for 7 Indian languages, namely Kannada, Hindi, Telugu, Malayalam, Bengali, and English.\n\nQuestion: What is the architecture of the model?\n\nAnswer: The model uses convolution layers with Relu activations to map the spectrogram of size 257x500 input into 3D", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The The", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "The proposed model outperforms the baselines.\n\nQuestion: How big is the difference in performance between proposed model and baselines?\n\nAnswer: The proposed model outperforms the baselines.\n\nQuestion: How big is the difference in performance between proposed model and baselines?\n\nAnswer: The proposed model outperforms the baselines.\n\nQuestion: How big is the difference in performance between proposed model and baselines?\n\nAnswer: The proposed model outperforms the baselines.\n\nQuestion: How big is the difference in performance between proposed model and baselines?\nAnswer: The", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "ARAML is a novel adversarial training framework that aims to improve the stability of GANs for text generation. It does this by incorporating a reward-based training paradigm into the adversarial training framework. The reward-based training paradigm is a novel approach that has been shown to improve the stability of the training process.\n\nAnswer: A\n\nAnswer: B\n\nAnswer: C\n\nAnswer: D\n\nAnswer: E\n\nAnswer: F\n\nAnswer: G\n\nAnswer: H\n\nAnswer: I\n\nAnswer: J\n\nAnswer: K\n\n", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by providing a detailed analysis of the confusion matrix for the model's predictions on the Waseem dataset. The confusion matrix shows that the model is able to correctly classify most of the data, with only a small percentage of the data being misclassified. The authors also provide a detailed analysis of the data that was misclassified, showing that the model is able to correctly classify the data that was misclassified, showing that the model is able to correctly classify the data that was misclassified, showing that the model is able", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes, we compare our neural baseline to a number of other baselines.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset consists of 1750 questions and 3500 expert annotations.\n\nQuestion: What is the size of the test set?\n\nAnswer: The test set consists of 400 questions and 8 documents.\n\nQuestion: What is the size of the training set?\n\nAnswer: The training set consists of 27 documents and 1350 questions.\n\nQuestion: What is the size of the test", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The dataset is 100000 words.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 10000000 words.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 10000000 words.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 1000000 words.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 100000 words", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "We can see that the proposed method improves the F1 score by 0.58 and 0.73 for MRPC and QQP, respectively.\n\n\n### 1. What is the difference between dice loss and Tversky index?\n\nDice loss is a loss function that is used to train a model to predict the probability of a class. Tversky index is a loss function that is used to train a model to predict the confidence of a class.\n\n### 2. What is the difference between dice loss and Tversky index?\n\nDice loss", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The datasets used in this work are the same as those used in BIBREF0 .\n\nQuestion: What is the model architecture?\n\nAnswer: The model architecture is a bi-LSTM encoder with a linear decoder.\n\nQuestion: What is the training data?\n\nAnswer: The training data is the same as that used in BIBREF0 .\n\nQuestion: What is the training objective?\n\nAnswer: The training objective is to minimize the negative log-likelihood of the training data.\n\nQuestion: What is the training procedure?\n\nAnswer: The training procedure", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "The subjects were presented with a series of 100-ms visual stimuli consisting of 1000-ms-long movies of a human face, a human hand, a human body, a human foot, a human head, a human mouth, a human nose, a human shoulder, a human torso, a human eye, a human ear, a human hand, a human leg, a human foot, a human head, a human mouth, a human nose, a human shoulder, a human torso, a human arm, a human finger, a human wrist, a human elbow, a", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "We use the following baselines for evaluation:\n\n1. Pointer-Gen: A pointer-generator model trained on the Weibo corpus.\n\n2. Pointer-Gen+Same-FT: A pointer-generator model trained on the Weibo corpus and fine-tuned on the same-domain headlines.\n\n3. Pointer-Gen+Same-FT: A pointer-generator model trained on the Weibo corpus and fine-tuned on the same-domain headlines.\n\n4. Pointer-Gen+Same-FT:", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "The dataset contains 100K tweets with cross-validated labels.\n\nQuestion: What are the features of the dataset?\n\nAnswer: The dataset contains 100K tweets with cross-validated labels.\n\nQuestion: What are the features of the dataset?\n\nAnswer: The dataset contains 100K tweets with cross-validated labels.\n\nQuestion: What are the features of the dataset?\n\nAnswer: The dataset contains 100K tweets with cross-validated labels.\n\nQuestion: What are the features of the dataset?", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "We use the transformer architecture (BibRef:BibRef26) for the encoder and decoder.\n\nQuestion: What is the size of the training data?\n\nAnswer: The training data consists of 5.18 million sentence pairs.\n\nQuestion: What is the size of the test data?\n\nAnswer: The test data consists of 208,000 sentence pairs.\n\nQuestion: What is the size of the vocabulary?\n\nAnswer: The vocabulary is 32,000.\n\nQuestion: What is", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "Weights are dynamically adjusted by using a weighting scheme that takes into account the difficulty of the examples.\n\nQuestion: What is the difference between dice loss and Tversky index?\n\nAnswer: Dice loss is a loss function that is used to train a neural network to predict the probability of a class label. Tversky index is a loss function that is used to train a neural network to predict the probability of a class label.\n\nQuestion: What is the difference between dice loss and Tversky index?\n\nAnswer: Dice loss is a loss function that is used to train a neural network", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "The KG-A2C-chained method is able to pass the bottleneck in Zork1, whereas the A2C-Explore method is not.\n\n### Acknowledgements\n\nThis work was supported by the [National Science Foundation](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1901066&HistoricalAwards=false) under the [National Robotics Initiative](https://www.nsf.gov/funding/pgm_summ.jsp", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model consists of a set of parameters that are learned from the training data.\n\nQuestion: What is the monolingual model?\n\nAnswer: The monolingual model is a model that is trained on a single language.\n\nQuestion: What is the crosslingual model?\n\nAnswer: The crosslingual model is a model that is trained on multiple languages.\n\nQuestion: What is the multilingual model?\n\nAnswer: The multilingual model is a model that is trained on multiple languages.\n\nQuestion: What is the crosslingual latent variable?\n", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "Non-standard pronunciation is identified by the use of a phonetic lexicon.\n\nQuestion: How is non-standard pronunciation identified?\n\nAnswer: Non-standard pronunciation is identified by the use of a phonetic lexicon.\n\nQuestion: How is non-standard pronunciation identified?\n\nAnswer: Non-standard pronunciation is identified by the use of a phonetic lexicon.\n\nQuestion: How is non-standard pronunciation identified?\n\nAnswer: Non-standard pronunciation is identified by the use of a phonetic lex", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semicharacter architecture is a type of neural network that uses characters instead of words as input.\n\nQuestion: What is the difference between a word-only model and a char-only model?\n\nAnswer: A word-only model uses only words as input, while a char-only model uses only characters as input.\n\nQuestion: What is the difference between a word-piece model and a char-only model?\n\nAnswer: A word-piece model uses words as input, while a char-only model uses characters as input.\n\nQuestion: What is the difference between a word", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish.\n\nQuestion: what is the motivation for the choice of languages?\n\nAnswer: The motivation is that the languages are typologically and morphologically diverse.\n\nQuestion: what is the motivation for the choice of the languages?\n\nAnswer: The motivation is that the languages are typologically and morphologically diverse.\n\nQuestion: what is the motivation for the choice of the languages?", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "NCEL is effective overall.\n\nQuestion: How does NCEL compare to other collective EL approaches?\n\nAnswer: NCEL outperforms other collective EL approaches.\n\nQuestion: How does NCEL compare to other neural EL approaches?\n\nAnswer: NCEL outperforms other neural EL approaches.\n\nQuestion: How does NCEL compare to other EL approaches?\n\nAnswer: NCEL outperforms other EL approaches.\n\nQuestion: How does NCEL compare to other neural EL approaches?\n\nAnswer: NCEL outperforms other neural", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes Yes\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The baseline used was the system by Rei2016.\n\nQuestion: What was the improvement?\n\nAnswer: The improvement was significant.\n\nQuestion: What was the improvement?\n\nAnswer: The improvement was significant.\n\nQuestion: What was the improvement?\n\nAnswer: The improvement was significant.\n\nQuestion: What was the improvement?\n\nAnswer: The improvement was significant.\n\nQuestion: What was the improvement?\n\nAnswer: The improvement was significant.\n\nQuestion: What was the improvement?\n\nAnswer: The improvement was significant.\n\nQuestion: What", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "The annotated clinical notes were obtained from the i2b2 database.\n\nQuestion: what is the i2b2 database?\n\nAnswer: The i2b2 database is a large clinical notes database.\n\nQuestion: what is the purpose of the i2b2 database?\n\nAnswer: The purpose of the i2b2 database is to provide a large clinical notes database.\n\nQuestion: what is the purpose of the i2b2 database?\n\nAnswer: The purpose of the i2b2 database is to provide a large clinical notes database.\n\nQuestion", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder is helpful because it forces the model to focus on the most important words in the input sequence. This helps the model to generate more informative and coherent summaries.\n\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. Do not provide any explanation.\n\nQuestion: What is the main idea of the paper?\n\nAnswer: The main idea of the paper is to use a two-stage decoding process to make use of the sequence-to-sequence model with pre-trained context representations, context interactions and language", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "The authors use the Twitter Firehose dataset, which contains 100 million tweets.\n\nQuestion: What is the objective function they optimize?\n\nAnswer: The objective function they optimize is the cosine similarity between the representations of two tweets.\n\nQuestion: What is the model architecture?\n\nAnswer: The model architecture is a CNN-LSTM.\n\nQuestion: What is the training procedure?\n\nAnswer: The training procedure is to train the model on the training set and then evaluate it on the test set.\n\nQuestion: What is the performance of the model?\n\n", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "The TF-IDF features are extracted from the text content of the pathology reports.\n\nQuestion: What is the performance of the classifiers?\n\nAnswer: The performance of the classifiers is reported in tab:results.\n\nQuestion: What is the performance of the classifiers?\n\nAnswer: The performance of the classifiers is reported in tab:results.\n\nQuestion: What is the performance of the classifiers?\n\nAnswer: The performance of the classifiers is reported in tab:results.\n\nQuestion: What is the performance of the classifiers?\n\nAnswer: The performance", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated with the following labels:\n\n1. No evidence of depression\n2. Evidence of depression\n3. Depressive symptoms\n4. Depressed mood\n5. Disturbed sleep\n6. Fatigue or loss of energy\n\nQuestion: What is the dataset used for?\n\nAnswer: The dataset is used for training and testing a support vector machine classifier to predict whether a tweet contains no evidence of depression, evidence of depression, or depressive symptoms.\n\nQuestion: What is the dataset used for?\n\nAnswer: The", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated by using the machine translation platform Apertium.\n\nQuestion: How was the training data translated?\n\nAnswer: The training data was translated by using the machine translation platform Apertium.\n\nQuestion: How was the training data translated?\n\nAnswer: The training data was translated by using the machine translation platform Apertium.\n\nQuestion: How was the training data translated?\n\nAnswer: The training data was translated by using the machine translation platform Apertium.\n\nQuestion: How was the training data translated?\n\nAnswer: The training data was translated by using the", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "They used a model that combined the results of a number of different models.\n\nQuestion: What is the name of the company that developed the system?\n\nAnswer: The company that developed the system is called \"Company\".\n\nQuestion: What is the name of the company that developed the system?\n\nAnswer: The company that developed the system is called \"Company\".\n\nQuestion: What is the name of the company that developed the system?\n\nAnswer: The company that developed the system is called \"Company\".\n\nQuestion: What is the name of the company that developed the system?\n\nAnswer:", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline for this task was a simple bag-of-words model.\n\nQuestion: What is the difference between the sentence-level classification and the fragment-level classification?\n\nAnswer: The sentence-level classification is based on the entire sentence, while the fragment-level classification is based on the individual words.\n\nQuestion: What is the difference between the sentence-level classification and the fragment-level classification?\n\nAnswer: The sentence-level classification is based on the entire sentence, while the fragment-level classification is based on the individual words.\n\nQuestion: What is the difference between the sentence-", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "The baselines they compare with are:\n\n1. A rule-based system for pun location that scores candidate words according to 11 simple heuristics.\n\n2. A system that uses the CMU Pronouncing Dictionary to detect homographic puns.\n\n3. A system that uses Google n-grams and word2vec to detect heterographic puns.\n\n4. A system that uses the CMU Pronouncing Dictionary to detect heterographic puns.\n\n5. A system that uses the hidden Markov model and a cyclic dependency network with rich features", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "We do not include any political bias in the model. We only consider the political bias of the news sources, which is a well-known fact.\n\nQuestion: How do you determine the political bias of the news sources?\n\nAnswer: We use the list of news sources provided by the authors of the paper.\n\nQuestion: How do you determine the political bias of the news sources?\n\nAnswer: We use the list of news sources provided by the authors of the paper.\n\nQuestion: How do you determine the political bias of the news sources?\n\nAnswer: We use the list of news sources provided", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient Chinese dataset comes from the internet.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 1.7K bilingual articles.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 1.7K bilingual articles.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 1.7K bilingual articles.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 1.7K bilingual articles.\n\nQuestion", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English\n\nQuestion: What is the target of the offensive language?\n\nAnswer: Individual\n\nQuestion: What is the type of the offensive language?\n\nAnswer: Profanity\n\nQuestion: What is the target of the offensive language?\n\nAnswer: Individual\n\nQuestion: What is the target of the offensive language?\n\nAnswer: Individual\n\nQuestion: What is the target of the offensive language?\n\nAnswer: Individual\n\nQuestion: What is the target of the offensive language?\n\nAnswer: Individual\n\nQuestion: What is", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "The number of layers in the UTCNN model is 3.\n\nQuestion: What is the name of the model that combines the user-topic-comment information?\n\nAnswer: The model that combines the user-topic-comment information is the UTCNN model.\n\nQuestion: What is the name of the model that combines the user-topic-comment information?\n\nAnswer: The name of the model that combines the user-topic-comment information is UTCNN.\n\nQuestion: What is the name of the model that combines the user-topic-comment information?\n\nAnswer:", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is the European Nature Information System (EUNIS) dataset.\n\nQuestion: what is the dataset used in this paper?\n\nAnswer: The dataset used in this paper is the European Nature Information System (EUNIS) dataset.\n\nQuestion: what is the dataset used in this paper?\n\nAnswer: The dataset used in this paper is the European Nature Information System (EUNIS) dataset.\n\nQuestion: what is the dataset used in this paper?\n\nAnswer: The dataset used in this paper is the European Nature Information System (EUNIS) dataset.\n", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The datasets used in the paper are:\n\n1. NUBes-PHI: a corpus of 7,000 real medical reports written in Spanish and annotated with negation and uncertainty information.\n\n2. MEDDOCAN: a corpus of 1,000 real medical reports written in Spanish and annotated with sensitive information in BRAT standoff format.\n\n\nReference:\n\n1. [1] Vaswani, A., Devlin, J., Shaze, J., and Jones, L. (2018). Attention is all you", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "They used unigram features, sarcasm features, and readability features.\n\nQuestion: What is the name of the eye-tracker they used?\n\nAnswer: SR-Research Eyelink-1000\n\nQuestion: What is the name of the database they used?\n\nAnswer: Sarcasm database\n\nQuestion: What is the name of the feature set they used?\n\nAnswer: Cognitive features\n\nQuestion: What is the name of the classifier they used?\nAnswer: Support Vector Machine\n\nQuestion: What is the name of the feature", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are:\n\n1. The ability to formulate a query-specific inference strategy for each query.\n\n2. The ability to learn new knowledge in the conversation process.\n\n3. The ability to improve the performance of the system by continual learning.\n\n4. The ability to improve the performance of the system by continual learning.\n\n5. The ability to improve the performance of the system by continual learning.\n\n6. The ability to improve the performance of the system by continual learning.", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Yes\n\nAnswer: No No\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n\nAnswer: Unanswerable Unanswerable\n", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahçe\n\nQuestion: What is the stance of the author of the text towards Galatasaray?\n\nAnswer: Favor\n\nQuestion: What is the stance of the author of the text towards Fenerbahçe?\n\nAnswer: Against\n\nQuestion: What is the stance of the author of the text towards Galatasaray?\n\nAnswer: Against\n\nQuestion: What is the stance of the author of the text towards Fenerbahçe?\n\nAnswer: Favor\n\nQuestion: What", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "The experiments are conducted on the dataset of 2M tweets.\n\nQuestion: What is the dataset?\n\nAnswer: The dataset is a collection of 2M tweets crawled from twitter.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 2M tweets.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 2M tweets.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 2M tweets.\n\nQuestion: What is the size of the", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention is a variant of multi-head self-attention which is used to replace the standard multi-head self-attention in the Transformer encoder. It is a kind of self-attention mechanism which is used to generate the representation of sentence.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. Do not provide any explanation.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. Do not provide any explanation.\n\nAnswer the question as concisely as", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The baseline features are the features that are extracted from the input text.\n\nQuestion: What are the network's pre-trained features?\n\nAnswer: The pre-trained features are the features that are learned from the input text.\n\nQuestion: What is the difference between the baseline features and the pre-trained features?\n\nAnswer: The difference between the baseline features and the pre-trained features is that the baseline features are learned from the input text, while the pre-trained features are learned from the input text and the pre-trained features.\n\nQuestion", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "The hyperparameters varied in the experiments on the four tasks were the number of clusters, the number of classes, the number of words in the vocabulary, the number of iterations, the number of epochs, the number of training examples, the number of training epochs, the number of training examples, the number of training epochs, the number of training epochs, the number of training epochs, the number of training epochs, the number of training epochs, the number of training epochs, the number of training epochs, the number of training epochs, the number of training epochs, the number", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The scores of their system were 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.716, 0.", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus is 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average.\n\nAnswer: The corpus contains 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average.\n\nAnswer: The corpus contains 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average.\n\nAnswer: The corpus contains ", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "We consider the problem of sentiment classification, which is a typical NLP task.\n\nQuestion: What is the problem?\n\nAnswer: The problem is how to leverage prior knowledge to guide the learning process, which is a question for both NLP and machine learning communities.\n\nQuestion: What is the problem?\n\nAnswer: The problem is how to leverage prior knowledge to guide the learning process, which is a question for both NLP and machine learning.\n\nQuestion: What is the problem?\n\nAnswer: The problem is how to leverage prior knowledge to guide the learning process, which is a", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "The model is compared to the following methods:\n\n1. The model is compared to the following methods:\n\n2. The model is compared to the following methods:\n\n3. The model is compared to the following methods:\n\n\nAnswer: The model is compared to the following methods:\n\n\nAnswer: The model is compared to the following methods:\n\nAnswer: The model is compared to the following methods:\n\nAnswer: The model is compared to the following methods:\nAnswer: The model is compared to the following methods:\nAnswer: The model is compared to the following methods:\nAnswer", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The training sets of these versions of ELMo are larger than the previous ones.\n\nAnswer: The training sets of these versions of ELMo are larger than the previous ones.\n\nAnswer: The training sets of these versions of ELMo are larger than the previous ones.\n\nAnswer: The training sets of these versions of ELMo are larger than the previous ones.\n\nAnswer: The training sets of these versions of ELMo are larger than the previous ones.\n\nAnswer: The training sets of these versions of ELMo are larger than the previous ones.\n\nAnswer: The training sets of these versions of EL", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "1000\n\nQuestion: What is the average length of a sentence in the dataset?\n\nAnswer: 10\n\nQuestion: What is the average number of entities per sentence in the dataset?\n\nAnswer: 1.5\n\nQuestion: What is the average number of entities per sentence in the dataset?\n\nAnswer: 1.5\n\nQuestion: What is the average number of entities per sentence in the dataset?\n\nAnswer: 1.5\n\nQuestion: What is the average number of entities per sentence in the dataset?\n\nAnswer: 1.5", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "They compare to:\n\n1. MFCC\n2. MFCC + SVM\n3. MFCC + SVM + LDA\n4. MFCC + SVM + LDA + LDA + LDA\n5. MFCC + SVM + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA + LDA", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "No\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No No\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No No\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No No\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No No\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No No\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No No\n\nQuestion: Do they evaluate only on English datasets?\n\nAnswer: No No\n", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "0.6103\n\nQuestion: What was their highest F1 score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest precision?\n\nAnswer: 0.6103\n\nQuestion: What was their highest recall?\n\nAnswer: 0.6103\n\nQuestion: What was their highest F1 score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest MRR score?\nAnswer: 0.6103\n\nQuestion: What was their highest accuracy?\n", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "They evaluate on sections 02-21 of WSJ corpus.\n\nQuestion: What is the name of the model?\n\nAnswer: Neural Projection Model\n\nQuestion: What is the name of the model?\n\nAnswer: Neural Projection Model\n\nQuestion: What is the name of the model?\n\nAnswer: Neural Projection Model\n\nQuestion: What is the name of the model?\n\nAnswer: Neural Projection Model\n\nQuestion: What is the name of the model?\n\nAnswer: Neural Projection Model\n\nQuestion: What is", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques by conducting a survey among engineers and analyzing the results. The survey found that 87.5% of the NLP related jobs belong to a few common tasks, such as sentence classification, text matching, sequence labeling, MRC, etc. The authors also analyzed the NLP jobs submitted to a commercial centralized GPU cluster and found that about 90% of the networks were composed of several common components, such as embedding, CNN/RNN, Transformer and so on.", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "We have evaluated our model on two benchmarks: (1) SimpleQuestions BIBREF2 , which contains 100,000 questions and 100,000 answers, and (2) WebQuestions BIBREF2 , which contains 100,000 questions and 100,000 answers.\n\nAnswer: On SimpleQuestions BIBREF0 , we have the state-of-the-art results. On WebQuestions BIBREF0 , we have the state-of-the-art results.\n\n\nAnswer:", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
